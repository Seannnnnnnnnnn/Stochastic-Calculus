\documentclass{article}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{natbib} 
\usepackage{tcolorbox} 
\usepackage{amsmath}
\usepackage{amssymb}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }


\begin{document}

% Cover Page
\begin{titlepage}
    \centering
    {\Huge \textbf{Stochastic Calculus}}\\[1.5cm] % Title
    \textbf{Author:} Sean Conlon\\[1cm] % Replace n'Your Name' with 
    \textbf{Student Id:} 1298865\\[1cm]
    \textbf{Date:} \today\\[3cm] % Automatically inserts today's date
    
    \section*{Assignment 1.}
    This document contains my solution for assignment 1 of University of Melbourne's \texttt{MAST90059 Stochastic Calculus} subject.

    % Table of Contents
    \tableofcontents
    
\end{titlepage}

% Example sections (these will appear in the table of contents)
\newpage
\section{Question 1}

\begin{tcolorbox}
[colframe=black,colback=gray!5,boxrule=0.5pt]
Let $B_t$ be a standard Brownian Motion. For any fixed $c > 0$ and $n\geq 3$ we define the following sets
\begin{align*}
    & A_n := \{\omega : \exists s\in[0,1] :|B_t(\omega) - B_s(\omega)|\leq c|t-s| \text{ if } |t-s|\leq 3/n\} \\
    & L_{k,n} := \max_{j=0,1,2} \{|B_{(k+j)/n} - B_{(k+j-1)/n}|\}, \quad 1\leq k\leq n-2 \\
    & D_n  := \{\text{at least one of the $L_{k,n}$'s is }\leq 5c/n\}
\end{align*}
\end{tcolorbox}

\begin{tcolorbox}
[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{a)} Show that $A_n\subseteq D_n$
\end{tcolorbox}
\textbf{Proof}. Assume $\omega\in A_n$. Then by definition, there exists some $s\in[0,1]$ such that for all $t$ with $|t-s|\leq 3/n$ we have 
$$|B_{t}(\omega) - B_s(\omega)|\leq c|t-s|$$
To locate the interval containing $s$, we partition $[0,1]$ into $n$ sub-intervals of length $\frac{1}{n}$. The point $s$ must lie in one of the sub-intervals, say 
$$s\in\left[\frac{m-1}{n}, \frac{m}{n}\right)\quad \text{ for some } m\in\{1,2,\dots,n\}$$ 
To ensure the neighborhood $|t-s|\leq 3/n$ stays within $[0,1]$ we assume $s\in[2/n, 1 -2/n]$. We'll handle the boundary cases separately at the end. \\
\\
We now consider the three consecutive intervals centered at $m$: 
$$\left[\frac{m-2}{n}, \frac{m-1}{n}\right], \quad \left[\frac{m-1}{n}, \frac{m}{n}\right], \quad \left[\frac{m}{n}, \frac{m+1}{n}\right]$$
For any $t$ in these intervals, $|t-s|\leq 3/n$ since $s\in[\frac{m-1}{n}, \frac{m}{n})$. Now using this fact we have: 
\begin{enumerate}
    \item For $t=\frac{m-1}{n}$: $$|B_{\frac{m-1}{n}}(\omega) - B_s(\omega)|\leq c\left(s - \frac{m-1}{n}\right) \leq \frac{c}{n}$$

    \item for $t = \frac{m}{n}$: 
    $$|B_{\frac{m}{n}}(\omega) - B_s(\omega)|\leq c\left(s - \frac{m}{n}\right) \leq \frac{c}{n}$$

    \item for $t = \frac{m+1}{n}$:
    $$|B_{\frac{m+1}{n}}(\omega) - B_s(\omega)|\leq c\left(s - \frac{m+1}{n}\right) \leq \frac{2c}{n}$$
\end{enumerate}
We can now bound the increments: 
\begin{align*}
     & |B_{\frac{m-1}{n}} - B_{\frac{m-2}{n}}| \leq |B_{\frac{m-1}{n}} 
 + s| + | s - B_{\frac{m-2}{n}}| \leq \frac{2c}{n} \\
  & |B_{\frac{m+1}{n}} - B_{\frac{m}{n}}| \leq |B_{\frac{m+1}{n}} - s | + |s - B_{\frac{m}{n}}| \leq \frac{2c}{n} + \frac{c}{n} = \frac{3c}{n}
\end{align*}
Thus, for $k=m-1$: 
$$L_{m-1, n} = \max \left\{ |B_{\frac{m-1}{n}} - B_{\frac{m-2}{n}}|, |B_{\frac{m}{n}} - B_{\frac{m-1}{n}}|, |B_{\frac{m+1}{n}} - B_{\frac{m}{n}}|\right\}\leq \frac{5c}{n}$$
In the event that $s\in[0, \frac{2}{n})$ or $s\in(1 - \frac{2}{n}, 1]$ we adjust to $k=1$ or $k=n-2$ respectively and repeat the above argument. We conclude that in either case, for every $\omega\in A_n$ there exists a $k$ such that $L_{k,n}(\omega)\leq \frac{5c}{n}$ and thus $A_n\subseteq D_n$. $\square$

\newpage
\begin{tcolorbox}
[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{b)} Show that $\mathbb{P}(D_n)\to0$ as $n\to\infty$.
\end{tcolorbox}
\textbf{Proof.} Since $D_n$ is the event that \textit{at least one} $L_{k,n}\leq\frac{5c}{n}$ we have: 
$$\mathbb{P}(D_n) \leq \sum_{k=1}^{n-1}\mathbb{P}\left( L_{k,n} \leq \frac{5c}{n}\right )$$
For a fixed $k$, $L_{k,n}$ is the maximum of the three independant Brownian Increments: 
$$L_{k,n} = \max \left\{ |B_\frac{k}{n} - B_\frac{k-1}{n}|, |B_\frac{k+1}{n} - B_\frac{k}{n}|, |B_\frac{k+2}{n} - B_\frac{k+1}{n}|\right\}$$
Notice that each increment follows a $N(0, \frac{1}{n})$ distribution. Hence, we can estimate the probability that a single increment is $\leq \frac{5c}{n}$ by
\begin{align*}
    \mathbb{P}\left(|B_\frac{k+j}{n} - B_\frac{k+j-1}{n}| \leq \frac{5c}{n}\right) &= \mathbb{P}\left( |Z|\leq \frac{5c}{\sqrt{n}.} \right) \\
    &= 2\Phi\left(\frac{5c}{\sqrt{n}}\right) - 1 \\
    &\approx \frac{10c}{\sqrt{2\pi n}} \hspace{15mm} (\text{for small $x$})
\end{align*}
And since three the increments are independent we have 
\begin{align*}
    \mathbb{P}\left(L_{k,n}\leq \frac{5c}{n}\right) = \mathbb{P}\left( |Z|\leq \frac{5c}{\sqrt{n}.} \right)^3 &\leq \left(\frac{10c}{\sqrt{2\pi n}}\right)^3 \\
    &= O(n^{-3/2})
\end{align*}
Hence our original inequality becomes 
$$\mathbb{P}(D_n) \leq (n-2) O(n^{-3/2}) = O(n^{-1/2})$$
as $n\to\infty$, $O(n^{-1/2})$ tends to 0 so we conclude $\mathbb{P}(D_n)\to0.$ $\square$

\begin{tcolorbox}
[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{c)} Show that $\mathbb{P}(A_n) = 0$ for all $n$.
\end{tcolorbox}
Our previous result shows that $\mathbb{P}(A_n) \to0$ asymptotically, but now we wish to show that $A_n$ is $\mathbb{P}$-null set for all $n$. To do so, we first note that $A_n$ is non increasing, that is $A_{n+1}\subseteq A_n$. It should be clear that this true because for $n+1$, the condition becomes stricter: 
$$|B_t - B_s|\leq c|t-s| \quad \text{ for } \quad |t-s|\leq \frac{3}{n+1}\subsetneq \frac{3}{n}$$
Thus if $\omega\in A_{n+1}$ holds on a smaller neighborhood, then $\omega \in A_n$ also. Since $A_n \supseteq A_{n+1}$ the sequence $\{A_n\}$ is non-increasing and: 
$$\mathbb{P}\left(\bigcap_{n=3}^{\infty}A_n\right) = \lim_{n\to\infty}\mathbb{P}(A_n) = 0.$$
Finally, since $\cap_{n=3}^{\infty} A_n\subseteq A_n$ for every $n$ we have 
$$0\leq \mathbb{P}(A_n)\leq \mathbb{P}\left(\bigcap_{n=3}^{\infty}A_n\right) = 0$$
to conclude that $\mathbb{P}(A_n)=0$ for all $n$. $\square$

\newpage
\begin{tcolorbox}
[colframe=black,colback=gray!5,boxrule=0.5pt]
 \textbf{d)} In view of the definition of Lipschitz condition, what conclusion can we make from (c)?
\end{tcolorbox}
\textbf{Answer.} We conclude that Brownian Motion is nowhere locally Lipschitz, and thus nowhere differentiable almost surely. 

\newpage
\section{Question 2}

\begin{tcolorbox}
[colframe=black,colback=gray!5,boxrule=0.5pt]
Let $B_t$ be a standard Brownian Motion and $N_t$ be a Poisson process with rate $\frac{\sqrt{e}}{2(\sqrt{e}- 1)}$ where $B_t$ and $N_t$ are independent. Define 
$$X_t := \sqrt{t} e^{-\frac{1}{2}N_{\ln t}} B_{e^{N_{\ln t}}} \hspace{5mm} t\geq 1$$
\end{tcolorbox}


\begin{tcolorbox}
[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{a)} Show that $X_t \sim N(0,t)$ for $t\geq 1$
\end{tcolorbox}
\textbf{Proof.} We first note for $t\geq1$ the process $N_{\ln t}$ is well defined. Since the Brownian Motion in $X_t$ is indexed at $e^{N_{\ln t}}$ we first consider the conditional distribution $X_t | N_{\ln t} =n$. The process is now comprised of
\begin{itemize}
    \item The Brownian Motion term which now has distribution $B_{e^{N_{\ln t}}} = B_{e^n} \sim N(0, e^n)$. 
    \item A now deterministic decay term $e^{-\frac{1}{2}N_{\ln t}} = e^{-\frac{n}{2} }$
    \item A deterministic scaling term $\sqrt{t}$.
\end{itemize}
Combine these three and we now see that 
\begin{align*}
    X_t | \{N_{\ln t} = n\} &= \sqrt{t}e^{-\frac{n}{2}}B_{e^n} \\
    &\stackrel{d}{=} \sqrt{t} N(0,1) \\
    &\stackrel{d}{=} N(0,t)
\end{align*}
So we conclude $X_{t} | N_{\ln t}=n\sim N(0, t).$ Finally, we note that as this holds for arbitrary $n$ we must have the unconditional distribution as $X_t \sim N(0, t)$ also. $\square$


\begin{tcolorbox}
[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{b)} Let $1 \leq s < t$ and show that $\mathbb{E}[X_tX_s] = s$. Deduce the Covariance function of $\{X_t\}_{t\geq0}$. 
\end{tcolorbox}
\textbf{Proof.} To begin, we have 
$$\mathbb{E}[X_tX_s] = \sqrt{ts}\mathbb{E}[e^{-\frac{1}{2}(N_{\ln t} + N_{\ln s})}B_{e^{N_{\ln t}}}B_{e^{N_{\ln s}}}] \quad (1)$$
To neaten up the notation, we define $A = N_{\ln s}$, $D = N_{\ln t} + N_{\ln s}$. One can then verify that $N_{\ln t} = A + D$. Using these, we write 
$$(1) = \sqrt{ts}\mathbb{E}[e^{-\frac{1}{2}(2A+D)}B_{e^A}B_{e^{A+D}}] \quad (2)$$
Let's turn out attention to the product of Brownian Motion terms. Using the usual decomposition trick we have 
\begin{align*}
    B_{e^A}B_{e^{A+D}} &= B_{e^A}(B_{e^A} + (B_{e^{A+D}} - B_{e^A})) \\
    &= B_{e^A}^2 + B_{e^A}(B_{e^{A+D}} - B_{e^A})
\end{align*}
By the property of independent increments, we know that we take the expectation through the above, all that will be left is $\mathbb{E}[B_{e^A}^2] = e^A$. We now rewrite (2) as 
\begin{align*}
    \sqrt{ts}\mathbb{E}[e^{-\frac{1}{2}(2A+D)}B_{e^A}B_{e^{A+D}}] &= \sqrt{ts}\mathbb{E}[e^{-\frac{1}{2}(2A+D)}e^A] \\
    &= \sqrt{ts}\mathbb{E}[e^{-\frac{D}{2}}]
\end{align*}
The final step is to reason that since $D\sim\text{Poisson}(\lambda(\ln t - \ln s))$ we know use the Poisson MGF: 
$$\mathbb{E}[e^{kD}] = e^{\lambda(\ln t - \ln s)(e^{k} -1)}$$
substituting in our values for $k$ and $\lambda$ we find 
\begin{align*}
    \mathbb{E}[e^{-D/2}] = e^{\ln(t/s) \frac{\sqrt{e}}{2(\sqrt{e}-1)}(e^{-1/2} - 1)} &= e^{-\frac{1}{2}\ln (t/s)}  \\
    &= \left(\frac{t}{s}\right)^{-1/2} = \sqrt{\frac{s}{t}}
\end{align*}
Substituting this back in we find
$$\mathbb{E}[X_t X_s] = \sqrt{st} \sqrt{\frac{s}{t}} = s.$$
Finally, since $\mathbb{E}[X_t] = \mathbb{E}[X_s] = 0$ we find Cov$(X_t, X_s) = s$ for $1 \leq s < t$. $\square$

\begin{tcolorbox}
[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{c)} Let $\tau := \inf\{t>1 : N_{\ln t} - N_{\ln t-} = 1\}$. Explain why $X_{\tau}\neq X_{\tau-}$ a.s. Is $\{X_t\}_{t\geq 1}$ a Brownian Motion? Is it a Gaussian Process on $[1, \infty)$? 
\end{tcolorbox}
\textbf{Solution.} We first show that $X_\tau \neq X_{\tau -}$ a.s. We first note that $\tau$ is the time of the first \textit{jump} of the Poisson process. Thus, at time $t=\tau$ just before the jump the left limit satisfies $N_{\ln\tau-} = N_{\ln \tau} -1$ thus 
$$X_{\tau -} = \sqrt{\tau} e^{-\frac{1}{2}(N_{\ln\tau}-1)}B_{e^{N_{\ln\tau -1}}}$$
Similarly, when considering the right-side for $X_{\tau}$ at the first jump time we have $N_{\ln \tau} = N_{\ln \tau-}+1$. And thus 
$$X_{\tau} = \sqrt{\tau} e^{-\frac{1}{2}(N_{\ln\tau})}B_{e^{N_{\ln\tau}}}$$
The key observation is to note that the for the Brownian Motion term at the time $t=\tau$, it is evaluated at
$$e^{N_{\ln\tau}} = e^{N_{\ln\tau-}+1} = e\cdot e^{N_{\ln\tau-}}$$
This means that the Brownian Motion is evaluated at a new, independant time point. Since $B_{e^{N_{\ln\tau}}}$ and $B_{e^{N_{\ln \tau-}}}$ are independant (due to independant increments), the two values $X_{\tau}$ and $X_{\tau-}$ are not equal almost surely. \\
\\
Though $X_t$ shares the same marginal distribution and covariance structure of Brownian Motion, we have shown that $X_t$ has a discontinuity at time $\tau$ and thus, cannot be a Brownian Motion on $t\geq 1$. This also precludes $X_t$ from being a Gaussian Process, as if it were, it would imply that $X_t$ is a Brownian Motion which would contradict theorem 3.3 of \cite{Fima}. $\square$


\newpage
\section{Question 3}
\begin{tcolorbox}
[colframe=black,colback=gray!5,boxrule=0.5pt]
Let $B_t$ be a standard Brownian Motion and let $f : [0,\infty) \to [0,\infty)$ be an increasing continuous function with $f(0)=0.$ Define $X_t := B_{f(t)}$. Derive the quadratic variation $[X,X](t)$ directly from the definition. \\
\textbf{Hint:} One can compute the $L_2$-limit of the sequence of partial sums in the definition of quadratic variation.
\end{tcolorbox}
\textbf{Solution}. Recall quadratic variation is defined as 
$$[X,X](t) = \lim_{\|\Pi\|\to0}\sum_{i=1}^{n}(X_{t_i} - X_{t_{i-1}})^2$$
where $\Pi = \{0=t_0 < t_1<\dotsm < t_n=t\}$ is a partition of $[0,t]$ and $\|\Pi\| = \max_i |t_i - t_{i-1}|$ is the mesh size. We substitue our expression for $X_t$ into the definition to see
\begin{align*}
    \lim_{\|\Pi\|\to0}\sum_{i=1}^{n}(X_{t_i} - X_{t_{i-1}})^2 &= \lim_{\|\Pi\|\to0}\sum_{i=1}^{n}(B_{f(t_i)} - B_{f(t_{i-1})})^2
\end{align*}
Now recall that for standard Brownian Motion $B_t$ the quadratic variation over $[0,t]$ is $[B,B](t) = t$. That means for any partition $\Pi^\prime = \{0 = s_0 < s_1<\dotsm < s_m = t\}$ with mesh size $\|\Pi^\prime\|\to0$ 
$$\sum_{j=1}^{m}(B_{s_j} - B_{s_{j-1}})^2 \stackrel{L_2}{\rightarrow} t.$$
Now observe that under the time-transformation $f$ the partition $\Pi$ becomes (with some abuse of notation) $f(\Pi) := \{f(t_0),f(t_1),\dots,f(t_n)\}$. Since $f$ is continuous and increasing, $f(\Pi)$ is a valid partition of $[0, f(t)]$ and as $\|\Pi\|\to0$ the mesh size of $f(\Pi)\to0$ also because $f$ is uniformally continuous on $[0,t]$. Thus, the sum for $X_t$ becomes a sum over $f(\Pi)$ and 
$$\sum_{j=1}^{m}(B_{s_j} - B_{s_{j-1}})^2 \stackrel{L_2}{\rightarrow} [B,B](f(t)) = f(t)$$
Thus, we conclude that the $L_2$ - limit of the quadratic variation for $X_t = B_{f(t)}$ is $[X,X](t) = f(t)$. $\square$

\newpage
\section{Question 4}

\begin{tcolorbox}
[colframe=black,colback=gray!5,boxrule=0.5pt]
Let $B_t$ be a standard Brownian Motion. We define
\begin{align*}
    & T_0 := \inf\{s>0: B_s =0\} \\
    & L := \sup\{s\in[0,1]: B_s=0\}  \\
    & T_1 := \inf \{s>1: B_s=0\}
\end{align*}
We let $\mathbb{P}_x(\cdot) = \mathbb{P}(\cdot|B_0=x)$
\end{tcolorbox}

\begin{tcolorbox}
[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{a)} Show that $\mathbb{P}_x(T_1>1+t)=\int_{-\infty}^{\infty} \mathbb{P}_y(T_0>t)p_1(x,y)dy$ where $p_1(x,y)$ is the transition probability function for Brownian Motion at time $1$, given it started at $x$. 
\end{tcolorbox}
\textbf{Proof.} We note that the event $\{T_1 > 1+t\}$ means that the Brownian Motion does not hit 0 in the interval $(1, 1+t]$. To express this in terms of $B_1$ we condition on the position $y$ at time $t=1$: 
$$P_x(T_1 >1+t) = \int_{-\infty}^{\infty} P_x(T_1>1+t|B_1=y)p_1(x,y)dy$$
which we see is fairly close to our desired result. The final step is to apply the Markov Property of Brownian Motion: $P_x(T_1>1+t|B_1=y) = P_y(T_0>t)$. We substitute this into the integral to complete the proof; 
$$P_x(T_1 >1+t) = \int_{-\infty}^{\infty} P_x(T_1>1+t|B_1=y)p_1(x,y)dy = \int_{-\infty}^{\infty} \mathbb{P}_y(T_0>t)p_1(x,y)dy$$
$\square$


\begin{tcolorbox}
[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{b)} Show that $\mathbb{P}_x(L\leq t) = \int_{-\infty}^{\infty}\mathbb{P}_y(T_0>1-t)p_{t}(x,y)dy\quad t\in(0,1)$ 
\end{tcolorbox}
\textbf{Proof.} The event $\{L\leq t\}$ means that the last time Brownian Motion visits 0 in [0,1] occurs no later than $t$, which requires that $B_s$ doe not return to 0 in $(t,1]$. That is, $B_{t+s}\neq0$ for $s\in(0,1-t]$, which is equivalent to $T_0 >1-t$ for a Brownian Motion started at $y$. Thus, 
$$\mathbb{P}_x(L\leq t|B_t=y) = \mathbb{P}_y(T_0<1-t)$$
To complete the proof, the unconditional density is obtained by integrating over all possible values of $y$, that is
$$\mathbb{P}_x(L\leq t) = \int_{-\infty}^{\infty}\mathbb{P}_y(T_0>1-t)p_{t}(x,y)dy, \quad t\in(0,1)$$
$\square$

\begin{tcolorbox}
[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{c)} Show that $$P_x(B_t > y, T_0 \geq t) = \int_{y}^{\infty}(p_t(x,z) - p_t(x,-z))dz, \quad x,y > 0$$
\end{tcolorbox}
\textbf{Proof.} We first note that the event $\{B_t > y, T_0\geq t\}$ describes the path of Brownian Motion that are strictly positive on $[0,t]$ (which corresponds to $T_0\geq t$) and, end up above some level $y$ at time $t$ (which corresponds to $B_t\geq  y)$. From here, we can infer that is only sensible to consider positive $x$ and $y$, otherwise the event is not possible.  \\
\\
Our key observation is as follows; the probability $\mathbb{P}_x(B_t>y)$  includes paths that do and do not hit 0 before time $t$. The reflection principle states that the contribution to $\mathbb{P}_x(B_t>y)$ from paths that hit 0 before $t$ can expressed by reflecting the path about 0. That is, 
\begin{align*}
    \mathbb{P}_x(B_t>y,T_0\geq t) &= \mathbb{P}_x(B_t>y) - \mathbb{P}_x(B_t>y,T_0<t) \\
    &= \mathbb{P}_x(B_t>y) - \mathbb{P}_x(B_t<-y)
\end{align*}
What remains it to simply find a nice expression for these integrals. Firstly, we apply a change of variables to see that 
\begin{align*}
    \mathbb{P}_x(B_t<-y) &= \int_{-\infty}^{-y}p_t(x,z)dz = \int_{y}^\infty p_t(x,-z)dz
\end{align*}
And thus, 
\begin{align*}
    \mathbb{P}_x(B_t>y,T_0\geq t) &= \mathbb{P}_x(B_t>y) - \mathbb{P}_x(B_t<-y) \\
    &= \int_y^{\infty}p_t(x, z)dz - \int_{y}^\infty p_t(x,-z)dz \\
    &= \int_{y}^{\infty}(p_t(x,z) - p_t(x,-z))dz
\end{align*}
Which completes the proof for $x,y>0$. $\square$

\newpage
\bibliographystyle{plain} 
\bibliography{references}  % Assumes a references.bib file

\end{document}
