\documentclass{article}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{natbib} 
\usepackage{tcolorbox} 
\usepackage{amsmath}
\usepackage{amssymb}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }


\begin{document}

% Cover Page
\begin{titlepage}
    \centering
    {\Huge \textbf{Stochastic Calculus}}\\[1.5cm] % Title
    \textbf{Author:} Sean Conlon\\[1cm] % Replace 'Your Name' with your actual name
    \textbf{Date:} \today\\[3cm] % Automatically inserts today's date
    
    \section*{About}
    This document is a collection of exercises and solutions for problems in Stochastic Calculus. The exercises are primarily sourced from the University of Melbourne's \texttt{MAST90059 Stochastic Calculus} subject, though some are also taken from other relevant textbooks. Any mistakes are entirely my own. Please email me at \texttt{sc.[first].[last]@gmail.com} for any questions or amendments.\\[2cm]

    % Table of Contents
    \tableofcontents
    
\end{titlepage}

% Example sections (these will appear in the table of contents)
\newpage
\section{Ordinary Calculus}

\newpage
\section{Probability \& Measure Theory Fundamentals}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} For a random variable $X\geq0$ show that $\mathbb{E}[X] = \int_{0}^{\infty}P(X\geq x)dx$
\end{tcolorbox}
\textbf{Proof.} For $X\geq0$ We have 
\begin{align*}
    E[X] = \int_{0}^{\infty}xf(x)dx &= \int_{0}^{\infty} x dF(x) \\
    &= \int_{0}^{\infty}\int_{0}^{x}dtdF(x)
\end{align*}
We now wish to apply Fubini's theorem to change the order of integration. Notice that the region of integration runs over $0\leq t\leq x<\infty$ so after applying Fubini's theorem
\begin{align*}
    \int_{0}^{\infty}\int_{0}^{x}dtdF(x) &= \int_{0}^{\infty}\int_{x}^{\infty}dF(x)dt \\
    &=\int_{0}^{\infty}(1 - F(t))dt \\
    &= \int_{0}^{\infty} P(X\geq t)dt \hspace{10mm}
    \square\\
\end{align*}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} (A common misconception) Show that two normally distributed and uncorrelated random variables are not necessarily independent. 
\end{tcolorbox}
\textbf{Proof.} Let $Z_1,Z_2\stackrel{iid}{\sim}N(0,1)$ and define $X = Z_1 + Z_2$ and  $Y = Z_1 - Z_2$. One verifies 
\begin{align*}
    \text{Cov}(X_, Y) = E[XY] &= E[Z_1^2 -Z_2^2] \\
    &= 0.
\end{align*}
So it is clear that $X$ and $Y$ are uncorrelated. We now wish to show that they are not independent. To derive the joint distribution of $X,Y$ we first consider the joint distribution of $Z_1, Z_2$ then apply a change of variables. As $Z_i$ are \textit{iid} their joint distribution is simply the product of the marginal distributions. That is
$$f_{Z_1, Z_2}(z_1, z_2) = \frac{1}{2\pi}e^{-\frac{1}{2}(z_1^2 + z_2^2)}.$$
Given the definitions of $X$ and $Y$ we now apply the change of variables 
$$Z_1 = \frac{X+Y}{2} \hspace{5mm} Z_2 = \frac{X-Y}{2}$$
One now considers 
\begin{align*}
    f_{X,Y}(x, y) &= f_{Z_1, Z_2}\left(\frac{x+y}{2}, \frac{x-y}{2}\right) \\
    &= \frac{1}{2\pi}e^{-\frac{1}{4}(x^2 + y^2)}.
\end{align*}
To verify that they are not independent, we now check the marginal distributions. 


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Suppose $(X,Y)$ has the following joint distribution
$$f_{X,Y}(x,y) = \frac{1}{2\pi\sqrt{1-\rho^2}}\exp\left(\frac{x^2 - 2\rho xy + y^2}{2(1-\rho^2)}\right)\quad x\in\mathbb{R},y\in\mathbb{R}$$
Find and interperate the Marginal distributions. [UniMelb]
\end{tcolorbox}
\textbf{Solution.} The critical observation is to factor the numerator in the exponent. We rewrite $$x^2 -2\rho xy + y^2 = (x-\rho y)^2 + (1-\rho^2)y^2$$
We can now rewrite the density function as
$$f_{X,Y}(x,y) = \frac{1}{2\pi\sqrt{1-\rho^2}}\exp\left(\frac{(x-\rho y)^2}{2(1-\rho^2)} - \frac{y^2}{2}\right)$$
We now integrate
\begin{align*}
    f_Y(y) &= \int_{X} f_{X,Y}dx \\
    &= \frac{1}{\sqrt{2\pi}}e^{-y^2/2}\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi(1-\rho^2)}}\exp\left(\frac{(x-\rho y)^2}{2(1-\rho^2)}\right)dx \\
    &\stackrel{d}{=} N(0,1)
\end{align*} 

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $T$ be a stopping time of the filtration $\mathcal{F}_t$. Define 
$$\mathcal{F}_T := \{A\in\mathcal{F}_t : A\cap\{T\leq t\}\in\mathcal{F}_t\}$$
Prove that $\mathcal{F}_T$ is a $\sigma$-algebra and $T$ is $\mathcal{F}_T$-measurable.
\end{tcolorbox}
\textbf{Proof.} To see that $\mathcal{F}_T$ is a $\sigma$-Algebra: 
\begin{enumerate}
    \item (Contains $\Omega$). Since $\Omega\in\mathcal{F}_t$ be definition and $\Omega \cap\{T\leq t\} = \{T\leq t\}\in\mathcal{F}_t$ we conclude that $\Omega\in\mathcal{F}_T$. 
    \item (Closure Under Compliments). Suppose $A\in\mathcal{F}_T$. Then $A^c\cap\{T\leq t\} = \{T\leq t\}\backslash A $
\end{enumerate}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} $X\sim N(\mu,\sigma^2)$ find the distribution function of $Y = |X|.$
\end{tcolorbox}
\textbf{Proof.}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Show that the characteristic function of a $N(\mu, \sigma^2)$ random variable is 
$$\varphi_X(t) = \exp(i\mu t - \sigma^2t^2/2)$$
\end{tcolorbox}
\textbf{Proof.}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} (Limits of Gaussian Random Variables are Gaussian) Let $\{Y_n:n\geq 0\}$ be a sequence of Gaussian Random variables such that $Y_n\stackrel{d}{\to} Y$. Show that $Y$ is either degenerate or Gaussian. 
\end{tcolorbox}
\textbf{Proof.}

\newpage
\section{Brownian Motion}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Show that $2B_{t} - B_s$ is not independent of $B_s$. \cite{Fima}
\end{tcolorbox}
\textit{Proof.} There are many ways to prove this, though our approach will show that $E[(2B_t-B_s)B_s]\neq E[2B_t-B_s] E[B_s]$ to include that the two terms are not independent. We have: 
\begin{align*}
    E[(2B_t-B_s)B_s] &= E[2B_tB_s - B_s^2] \\
    &= 2E[B_t B_s] - s
\end{align*}
Without loss of generality, we assume that $s<t$. We then apply the usual identity $B_t = B_s + (B_t - B_s)$ to the first term. This yields
\begin{align*}
    E[B_t B_s] &= E[(B_s + (B_t - B_s))B_s] \\
    &=E[B_s^2] + E[B_s(B_t - B_s)] \\
    &= s
\end{align*}
The last term is obtained by noting that $B_s \perp B_{t} - B_s$ (Independant Increments). We then substitute this into our orignal equation to see that 
$$E[(2B_t-B_s)B_s] = s \neq E[2B_t-B_s]E[B_s] \hspace{10mm} \Box$$

\vspace{2mm}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Find $P(B_t\leq0\text{ for } t=0,1,2)$ \cite{Fima}
\end{tcolorbox}
\textbf{Solution:} We assume standard Brownian Motion, so $B_0=0$. The problem is now to find $P(B_1\leq0, B_2\leq0)$.


\vspace{2mm}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $B_t$ denote a standard Brownian Motion. Which of the following are also Brownian Motion? \cite{Fima}
\begin{enumerate}
    \item $X_t = -B_t$
    \item $X_t = B_{2t} - B_t$
\end{enumerate} 
\end{tcolorbox}

\textbf{Solution:}\\
1) $X_t = -B_t$ defines a Brownian Motion. We can verify directly from the definition: 
\begin{itemize}
    \item (Indep. Increments). $X_t - X_s = -(B_t - B_s)$ which is independent of $X_u$ for $0\leq u <s$ from the definition of Brownian Motion. 
    \item (Normal Increments) $X_t - X_s = -(B_t - B_s) \sim N(0, t-s)$
    \item (Continuous Paths) $X_t$ is clearly continuous.
\end{itemize}
2) $X_t = t B_{1/t}$ also defines a Brownian Motion.

\vspace{2mm}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Find $\mathbb{P}(\int_0^1B_tdt > 2/\sqrt{3})$ \cite{Fima}
\end{tcolorbox}
\textbf{Solution:} $\mathbb{P}(\int_0^1B_tdt > 2/\sqrt{3}) = 1-\mathbb{P}(\int_0^1B_tdt \leq 2/\sqrt{3})$. From here, we see that if we can identify the distribution of $\int_{0}^{1}B_tdt$ then we we are pretty much done. To so so, we can approximate the integral by partitioning the interval into $0<t_1<t_2<\dots<1$, then
$$\int_{0}^{1}B_t dt \approx \sum_{i=1}^{n}B_{t_i}\Delta_{t_i} = \sum_{i=1}^{n}B_{t_i}(t_{i}-t_{_{i-t}})$$
We recall that $B_t$ is a Gaussian Process, and thus $\sum_iB_{t_i}\Delta_{t_i}$ is a linear combination of Gaussian processes and is therefore also a Gaussian Process. Finally we note that under taking the limit 
$$\lim_{\Delta\to0}\sum_{i=1}B_{t_i}\Delta_{t_i} = \int_{0}^{1}B_t dt$$
the limiting process is also Gaussian with equal mean. So far, we have $\int_{0}^{1}B_tdt\sim N(0, \sigma^2)$. To find the variance we consider
\begin{align*}
    \text{Var}\left(\int_{0}^{1}B_tdt\right) &= \text{Cov}\left(\int_{0}^{1}B_tdt\int_{0}^{1}B_sds\right) \\
    &= \mathbb{E}\left(\int_{0}^{1}B_tdt\int_{0}^{1}B_sds\right) \\
    &=\int_{0}^{1}\int_{0}^{1}\mathbb{E}[B_t B_s]dtds \hspace{10mm} \text{(Fubini)}\\
    &= \int_{0}^{1}\int_{0}^{1}\min(s, t)dtds
\end{align*}
To examine the integral, notice that the integral runs from $[0,t]$ when $s<t$ and $[0,s]$ when $t<s$. Drawing this out shows that they occupy the same area. Thus, 
\begin{align*}
    \int_{0}^{1}\int_{0}^{1}\min(s, t)dtds &= \int_{0}^{1}\int_{0}^{t}sdsdt + \int_{0}^{1}\int_{0}^{s}tdtds \\
    &= 2\int_{0}^{1}\int_{0}^{t}sdsdt \\
    &= \frac{1}{3}
\end{align*}
We conclude $\int_{0}^{1}B_tdt\sim N(0, 1/3)$.

\vspace{2mm}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Find the distribution of $B_t + B_s$ where $s < t$.
\end{tcolorbox}
\textbf{Solution.} It is important to note that $B_t$ is not independent of $B_s$ so we can't simply sum them as if they were independent normal variables. Instead 
\begin{align*}
    B_t + B_s = B_s + (B_t - B_s) + B_s &= 2B_s + (B_t-B_s) \\
    &\stackrel{d}{=}N(0, 4s) + N(0, t-s) \\
    &\stackrel{d}{=}N(0, t + 3s)
\end{align*}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $B_t$ and $W_t$ be two independant Brownian Motions. Show that the process $X_t = \frac{B_t + W_t}{\sqrt{2}}$ also defines a Brownian Motion. Find the correlation between $X_t$ and $B_t$. 
\end{tcolorbox}
\textbf{Solution.} It is straightforward to verify that $X_t$ is a Brownian Motion. To find the correlation recall that 
$$\text{Corr}(X_t, B_t) = \frac{\text{Cov}(X_t, B_t)}{\sqrt{\text{Var($X_t$)}}\sqrt{\text{Var($B_t$)}}} = \frac{\text{Cov}(X_t, B_t)}{t}$$
To find the covariance of the two processes
\begin{align*}
    \text{Cov}(X_t ,B_t) &= \frac{1}{\sqrt{2}}(E[B_t^2] + E[B_tW_t]) \\
    &= \frac{t}{\sqrt{2}}
\end{align*}
From here, we conclude Corr$(X_t, B_t) = 1/\sqrt{2}$

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Prove that $B_t$ is nowhere differentiable a.s. [UniMelb]  
\end{tcolorbox}
\textbf{Proof.} We begin by noting 
\begin{align*}
    \frac{B_{t+\delta} - B_t}{\delta} \stackrel{d}{=} \frac{\sqrt{\delta} Z}{\delta} \stackrel{d}{=} \frac{Z}{\sqrt{\delta}}
\end{align*}
It is now clear that for any $K > 0$ fixed
\begin{align*}
    \mathbb{P}\left(\left|\frac{Z}{\sqrt{\delta}}\right| > K \right) =1 \quad \text{as } \quad \delta\to0. 
\end{align*}
Thus with probability 1, we conclude that $B_t$ is nowhere differentiable. $\square$

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} (Fractal Scaling Property of Brownian Motion) Show that for any fixed $a>0$  the process $X_t = aB_{t/a^2}$ is itself a Brownian Motion.  
\end{tcolorbox}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} State and Prove the Markov Property for Brownian Motion.
\end{tcolorbox}
\textbf{Solution.} The a process satisfies the Markov Property (is Markovian) if the future trajectory of the process is independent of its past. That is, the future evolution of the process depends only on its current position, and no further history. Mathematically, 
$$P(X_{t+s} \leq y| \mathcal{F}_t) = P(X_{t+s} \leq y| X_t=x)$$
To see that Brownian Motion satisfies the Markov Property let $s <t$ and consider 
\begin{align*}
    P(B_t \leq y| \mathcal{F}_s) &= P(B_s + (B_t-B_s) \leq y|\mathcal{F}_s) \\
    &= P(B_t \leq y| B_s =x) \\
\end{align*}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} (Exit Time and Probability of Brownian Motion) 
\end{tcolorbox}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $s < t$, find the distribution of $B_s | B_t = b$.
\end{tcolorbox}
\textbf{Solution.} The \textit{trick} to this problem is to recognize that $(B_s, B_t)$ is multivariate normal then apply the \href{https://online.stat.psu.edu/stat505/lesson/6/6.1}{formulae for conditional distributions of multivariate Gaussians}. That is: 
\begin{align*}
    & E[B_s | B_t=b] = \frac{s}{t}b \\
    &\text{Var}(B_s | B_t=b) = s-\frac{s^2}{t} = \frac{s(t-s)}{t}
\end{align*}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Show that for any $0\leq a < b<\infty$ Brownian Motion is non-monotone on the interval $[a,b]$ with probability 1.  
\end{tcolorbox}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} By considering $M_t := \max_{0\leq s \leq t} B_s$ and $T_a$ be the first time that $B_t$ hits $a > 0$. Prove that $P(\tau_x\leq t) = P(|B_t|\geq x)$  
\end{tcolorbox}
\textbf{Proof.}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Show that $|B_t| \stackrel{d}{=} \max_{0\leq s\leq t} B_s$.
\end{tcolorbox}
\textbf{Proof.} For notional simplicity let $X_t := \max_{s\in[0,t]} B_s$ and define the stopping time for the first time $B_t$ hits the level $\alpha$. That is, $\tau_\alpha := \inf_{t>0}\{B_t = \alpha\}$. One can see that $\{X_t\geq\alpha\} = \{\tau_\alpha \leq t\}$. We can now calculate
\begin{align*}
    \mathbb{P}(\tau_\alpha \leq t) &= \mathbb{P}(\tau_\alpha\leq t, B_t \geq \alpha) + \mathbb{P}(\tau_\alpha\leq t, B_t < \alpha) \\
    &= 2\mathbb{P}(\tau_\alpha\leq t, B_t \geq \alpha) \\
    &= 2\mathbb{P}(X_t\geq \alpha, B_t\geq \alpha) \hspace{10mm}\text{{Substituting $\{X_t\geq\alpha\} = \{\tau_\alpha \leq t\}$}} \\
    &= 2\mathbb{P}(B_t\geq \alpha) \\
    &= \mathbb{P}(B_t\geq \alpha) + \mathbb{P}(B_t\leq -\alpha) \\
    &=\mathbb{P}(|B_t|\geq\alpha)
\end{align*}
Thus, we can see that $\mathbb{P}(X_t\geq\alpha) = \mathbb{P}(|B_t|\geq\alpha)$ to complete the proof. $\square$

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Prove the law of large numbers for Brownian Motion
$$\lim_{t\to\infty}\frac{B_t}{t} = 0 \text{ a.s.}$$
\end{tcolorbox}
\textbf{Proof.}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Prove the law of the iterated logarithm for Brownian Motion:
\begin{align*}
    & \limsup_{t\to\infty}\frac{B_t}{\sqrt{t\ln\ln t}}=1\quad \text{a.s} \\
    & \liminf_{t\to\infty}\frac{B_t}{\sqrt{t\ln\ln t}}=-1\quad \text{a.s}
\end{align*}
\end{tcolorbox}
\textbf{Proof.}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Argue that the process $X_t := \int_0^t B_sds$ exists. Show that it is a Gaussian Process, state its mean, variance and Covariance Function.
\end{tcolorbox}
\textbf{Proof.}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Show that the zeroes of Brownian Motion for an uncountable set which has zero measure $\mathbb{P}$ - almost surely.
\end{tcolorbox}
\textbf{Proof.}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Show that for any $x\geq 0$ 
$$\mathbb{P}(\max_{s\in[0,t]} B_t\geq x) = 2\left(1- \Phi\left(\frac{x}{\sqrt{t}}\right)\right).$$
\end{tcolorbox}
\textbf{Proof.} Define $\tau_x := \inf_{t\geq0}\{B_t=x\}$ and follow the usual steps for deriving the maximum of Brownian Motion: 
\begin{align*}
    \mathbb{P}(\tau_x \leq t) &= \mathbb{P}(\tau_\alpha\leq t, B_t \geq x) + \mathbb{P}(\tau_x\leq t, B_t < x) \\
    &= 2\mathbb{P}(\tau_x\leq t, B_t \geq x) \\
    &= 2\mathbb{P}(X_t\geq x, B_t\geq x) \hspace{10mm}\text{{Substituting $\{X_t\geq\alpha\} = \{\tau_\alpha \leq t\}$}} \\
    &= 2\mathbb{P}(B_t\geq x) \\
\end{align*}
We're now pretty much done. All that remains is to write this in the desired form
\begin{align*}
    2\mathbb{P}(B_t\geq x) = 2(1 - \mathbb{P}(B_t <x))  &= 2(1 - \mathbb{P}(B_t/\sqrt{t} <x/\sqrt{t})) \\
    &= 2\left(1- \Phi\left(\frac{x}{\sqrt{t}}\right)\right) \quad\square
\end{align*}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Show that for any $x<0$ 
$$\mathbb{P}(\min_{s\in[0,t]} B_t\leq x) = 2\mathbb{P}(B_t\leq x).$$
\end{tcolorbox}
\textbf{Proof.} 


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Find the joint density of $M_t := \max_{s\leq t} B_s$ and $B_t$.
\end{tcolorbox}
\textbf{Solution.} Let $y > x$ and define $\tau_y := \inf_{t}\{B_t \geq y\}$. As usual, note that $\{M_t \geq y\} = \{\tau_y\leq t\}$. Consider 
$$P(M_t\geq y, B_t\leq x) = P(\tau_y\leq t, B_t\leq x)$$
This means that, $B_t$ hits some level $y$ then descends back below $x$ on $\{\tau_y\leq t\}$. Recall that by the reflection principle, we now have 
\begin{align*}
    \{\hat{B}_t \leq x\} &= \{2B_{\tau} - B_t \leq x\} \\
    &= \{B_t\geq 2B_\tau - x\} \\
    &= \{B_t\geq 2y - x\}
\end{align*}
so, 
\begin{align*}
    P(\tau_y\leq t, B_t\leq x) &= P(\tau_y\leq t, B_t\geq2y- x) \\
    &= P(B_{t}\geq 2y-x) \\
    &= 1 - \Phi\left(\frac{2y-x}{\sqrt{t}}\right)
\end{align*}

\newpage
\section{Brownian Motion Calculus}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $X_t=-1$ on $0\leq t\leq1$, $X_t=1$ on $1 < t\leq2$ and $X_t=2$ on $2 < t\leq3$. Find the distribution of $\int_0^3X_sdB_s$. \cite{Fima}
\end{tcolorbox}
\textbf{Solution.} This is an Itô integral for a simple, nonstochastic process. By Definition
\begin{align*}
    \int_0^3 X_sdB_s &:= -(B_1-B_0) + (B_2-B_1)+2(B_3-B_2) \\ 
    &= 2B_3 -B_2-2B_1 \\
\end{align*}
We now the distribution will be gaussian with 0 mean. To find the variance we recall Cov$(B_t, B_s) = \min(t,s)$ and so
\begin{align*}
    \text{Var}(2B_3 -B_2-2B_1) &= \text{Var}(2B_3) + \text{Var}(B_2) + \text{Var}(2B_1) - 2\text{Cov}(B_3, B_2)-4\text{Cov}(B_3, B_2) +2\text{Cov}(B_2,B_1) \\
    &= 12 + 2 + 4-4-8+2 \\
    &= 6
\end{align*}
Hence we conclude $\int_0^3X_sdB_s\sim N(0,6).$


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} By finding a suitable sequence of Simple Adapted Processes, find $\int_0^T B_s dB_s$. \cite{Fima}
\end{tcolorbox}
\textbf{Solution.}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Find the values of $\alpha$ for which the process $Y_t = \int_0^t(t-s)^{-\alpha}dB_s$ is defined.
\end{tcolorbox}
\textbf{Solution}. Recall that $\int_0^t X_sdB_s$ is well-defined so long as $\int_0^t E[X_s^2]ds<\infty$. Applying this to $Y_t$ we require $\int_0^t(t-s)^{-2\alpha}ds <\infty$. Make the substitution $u = (t-s)$ then\footnote{Notice that $s$ ranges from $[0,t]$ in the original integrand, so when we make the $u$ substitution under the original end-points $u = t-s$ now varies from $t$ down to $0$. We then switch the order of these bounds which multiples through by $-1$.}
\begin{align*}
    \int_0^t(t-s)^{-2\alpha}ds &= \int_t^0 u^{-2\alpha}(-du) \\
    &= \int_0^tu^{-2\alpha}du
\end{align*}
From here, one clearly sees this integral is finite provided $\alpha < 1/2.$ $\square$

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Find the Stochastic Differential $d(Y_t / Z_t)$ where $Y_t = tB_t$ and $Z_t=e^{B_t}$ using Itô's formula. \cite{Fima}
\end{tcolorbox}
\textbf{Solution.} For exercises like this, the approach is: 
\begin{enumerate}
    \item Identify a suitable function $f$ to apply Itô's formula to. 
    \item Compute the partial derivatives of $f$. 
    \item Compute the differentials $dZ_t$ and $dY_t$. 
    \item Substitute everything in and cross cancel. 
\end{enumerate}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Derive the Stochastic Calculus equivalent of the Product Rule by applying Itô's lemma. 
\end{tcolorbox}
\textbf{Solution.} Consider the function $f(x,y) = xy$. One verifies it has the partial derivatives $f_x(x,y)=y$, $f_y(x,y)=x$, $f_{x,y}(x,y)=1$ and vanishing second-order derivatives. Applying Ito's formula for multi-variate functions one finds
$$dX_tY_t = Y_tdX_t + X_tdY_t + \sigma_X(t)\sigma_Y(t)dt.$$

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Suppose $X_t = (1-t)\int_0^t\frac{dB_s}{1-s}$, where $0\leq t<1$. Find $dX_t$
\end{tcolorbox}
\textbf{Solution.} This can appears a little intimidating, but the trick is to just realise it is a product of two component, and we can apply Itô's lemma to the function $f(x,y) = xy$.


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Find the quadratic variation of $X_t = tB_t$
\end{tcolorbox}
\textbf{Solution.} When asked to find the quadratic varition of a process, we should first ask if the process if an Itô process? If so, it can be written as $dX_t = \mu_t dt + \sigma_t dB_t$ and we immediately know 
$$[X, X]_t = \int_0^t\sigma_s^2ds.$$
For our process in question, we determine if it is an Itô process by applying Itô's lemma to the function $f(x,y) = xy$ with arguments $x=t, y=B_t$. 
\begin{align*}
    dX_t = df(t,B_t) &= \frac{\partial f}{\partial B_t}dB_t + \frac{\partial f}{\partial t}dt + \frac{\partial^2 f}{\partial B_t^2}dt \\
    &=tdB_t + B_tdt
\end{align*}
Thus, $X_t$ is an Itô process with $\sigma_t = t$. It follows that the quadratic variation is therefore
\begin{align*}
    [X,X]_t = \int_0^t s^2ds = \frac{t^3}{3} \quad \square
\end{align*}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $W_t$ be a standard Brownian Motion. Suppose $X_t$ is some process that satisfies $d(\frac{1}{X_t}) = \frac{1}{X_t}(2dt-dW_t)$. It is known that $dX_t = X_t(adt + bdW_t)$. Find $a$ and $b$.
\end{tcolorbox}
\textbf{Solution.} Applying Ito's Chain Rule to $f(t) = 1/t$ we have 
\begin{align*}
    d\left(\frac{1}{X_t}\right) = df(X_t) &=  f^\prime(X_t)dX_t + \frac{1}{2}f^{\prime\prime}(X_t)b^2X_t^2dt \\
    &= -\frac{1}{X_t^2}(aX_tdt + bX_tdW_t) + \frac{b^2}{X_t}dt \\
&= \frac{1}{X_t}(b^2-a)dt-bdW_t
\end{align*}
From here, it is clear that $2 = b^2-a$ and $-1 = b$. Thus, $a=b=-1$. $\square$

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $X_t = \int_0^t (ts)^2dB_s$. Find $dX_t$ and the quadratic variation $[X,X](t)$.
\end{tcolorbox}
\textbf{Solution.} Let $f(t,s) = t^2s^2$. By the Leibniz rule 
\begin{align*}
    dX_t &= f(t,t)dB_t + \int_0^t \frac{\partial}{\partial t}f(s,t) dB_sdt  \\
         &= t^4dB_t + \int_0^t 2ts^2dB_sdt \\
         &= t^4dB_t + 2t \left(\int_0^t s^2dB_s\right)dt
\end{align*}
For the Quadratic Variation, $X_t$ is an Itô process with $\sigma(s,t) = t^2 s^2$ so
$$[X,X]_t = \int_0^t \sigma(s,t)^2 ds = t^4\int_0^t s^4ds = \frac{t^9}{5}$$


\newpage
\section{Stochastic Differential Equations}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $\Phi$ be the standard normal cumulative distribution functoin. Show that the process $X_t := \Phi(\frac{B_t}{\sqrt{T-t}})$ for $0\leq t<T$ is a martingale for a fixed $T$.
\end{tcolorbox}
\textbf{Proof.}




























\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Show that $X_t = X_0 e^{(\mu-\sigma^2/2)t + \sigma B_t}$ satisfies $dX_t =\mu X_t dt + \sigma X_t dB_t$.
\end{tcolorbox}
\textbf{proof.} The "classic" way to show this is to start with the stochastic differential and apply Itô's lemma with $f(t) = \ln t$
\begin{align*}
    d\ln X_t  &= \frac{1}{X_t}(\mu X_t dt + \sigma X_t dB_t) - \frac{\sigma^2}{2}dt \\
              &= \left(\mu - \frac{\sigma^2}{2}\right)dt + \sigma dB_t
\end{align*}
Which implies by Itô's formula
\begin{align*}
    \ln X_t &= \ln X_0 + \left(\mu - \frac{\sigma^2}{2}\right)t + \sigma B_t \\
    &= X_0 \exp^{\left(\mu - \frac{\sigma^2}{2}\right)t + \sigma B_t}.
\end{align*}
Which completes the proof. $\square$

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Show that the solution to the \textit{Ornstein-Uhlenbeck} Process $dX_t = -\alpha X_tdt + \sigma dB_t$ is given by 
$$X_t = e^{-\alpha t}\left(X_0 + \int_0^t\sigma e^{\alpha s}dB_s\right)$$
\end{tcolorbox}
\textbf{proof.}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Show that the unique solution to $dU_t = U_tdX_t$ is given by 
$$\mathcal{E}(X_t) := U_t = e^{X_t - X_0 - \frac{1}{2}[X,X]_t}.$$
\end{tcolorbox}
\textbf{proof.}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Solve the SDE $dX_t = X_tB_tdt + X_tB_tdB_t, \quad X_0=1$.
\end{tcolorbox}
\textbf{Solution.} Start by writing
\begin{align*}
    dX_t &= X_tB_tdt + X_tB_tdB_t \\
    &= X_tdY_t
\end{align*}
Where $dY_t := B_tdt + B_tdB_t$. This is now in the form of a stochastic exponential which we know has unique solution
$$X_t = X_0\exp\left(Y_t - Y_0 + \frac{1}{2}[Y,Y]_t\right).$$
Since $Y_t$ is an Itô Process with $\sigma_s = B_s$
$$[Y,Y]_t = \int_0^t B_s^2 ds$$
Thus, the solution to the SDE is 
\begin{align*}
    X_t &= \exp\left( \int_0^tB_sds + \int_0^t B_sdB_s - \frac{1}{2}\int_0^t B_s^2ds\right). \\
    &= \exp\left( \int_0^tB_s - \frac{B_s^2}{2}ds + \int_0^t B_sdB_s \right). \quad \square
\end{align*}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Solve the SDE $dX_t = X_t dt + B_tdB_t, \quad X_0=1$.
\end{tcolorbox}
\textbf{Solution.} 


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Find $d\mathcal{E}(B_t)^2$ where $\mathcal{E}$ denotes the Stochastic Exponential.
\end{tcolorbox}
\textbf{Solution.} Recall that $\mathcal{E}(X_t) = \exp(X_t - X_0 + \frac{1}{2}[X,X]_t)$. So $\mathcal{E}(B_t) = \exp(B_t - t/2).$ We 

\newpage
\section{Diffusion Processes}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $\{X_t\}$ be a diffusion with $\mu(x) = 2x$ and $\sigma^2(x) = 4x$. Write the generator, solve $Lf=0$ and find a martingale.  
\end{tcolorbox}
\textbf{Solution.} The generator is given by
$$Lf(x) = 2x f^\prime(x) + 2xf^{\prime\prime}(x)$$
Setting $Lf=0$ and rearranging we find $f^{\prime \prime}(x) = -f^\prime(x)$. One can do some \textit{correct} things by applying some ODE theorems, but this isn't a class in Differential Calculus so I prefer to just notice that it has to be of the form $f(x) = e^{-x}$. One can easily verify this. Once accounting for possible constants we are left with 
$$f(x) = C_0 + C_1e^{-x}$$
as the solution. 


\newpage
\section{Change of Probability Measure}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Show that if $Q$ is equivalent to $P$ and $X\geq0$ then $E_PX > 0 $ implies $E_QX>0$ and vice-versa. 
\end{tcolorbox}
\textbf{Proof.} 


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Define $W_t = \mu t + B_t$ for a $\mathbb{P}$-Brownian Motion $B_t$. Define $W_t^*:=\max_{s\leq t} W_s$. Find the distribution of $W_t^*$
\end{tcolorbox}
\textbf{Solution.} The intuition is that we can't follow the same argument for finding the maximum of Brownian Motion, as $W_t$ is no longer a martingale. Instead, we'll apply a change of Measure to swap to probability measure where it is a martingale. 


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Give the Radon-Nikodym Derivative in the change to the EMM $\mathbb{Q}$ in the Black-Scholes model. 
\end{tcolorbox}
\textbf{Solution.} 




\newpage
\section{Applications to Finance}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} In the Binomial Pricing Model, show that the stock price can be expressed $S_t = \xi_tS_{t-1}$ where the $\xi_i$ are independent and identically distributed.   \cite{Fima}
\end{tcolorbox}
\textbf{Solution.}
Recall that in the Binomial Pricing Model, the stock can transition either $S_{t} \to uS_t$ or $S_t \to dS_t$. Define the random variable $\xi_i$ according to 
\[ \xi_i = \begin{cases} 
      u & \frac{u-r}{u-d} \\
      d & \frac{r-d}{u-d}
   \end{cases}
\]
Then it is clear that $S_{t+1} = S_0\prod_{i=1}^{t}\xi_i$. $\square$

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Show that the Binomial Model does not admit arbitrage $\iff$ $d<r<u.$
\end{tcolorbox}
\textbf{Proof.}
$(\Rightarrow)$. Suppose the model does not admit arbitrage. Then by the first fundamental theorem of asset pricing, there exists a probability measure $\mathbb{Q}$ such that the process $S_t / r^t$ is a martingale. Define $p:= \frac{u-r}{u-d}$ then we have 
\begin{align*}
    E_\mathbb{Q}\left[\frac{S_t}{r^t} | \mathcal{F}_{t-1}\right]
\end{align*}
 
\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Define what it means for a Market-Model to be \textit{Complete}. Show that the Binomial Model is complete. 
\end{tcolorbox}
\textbf{Solution.}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Give an example of a model which is not complete. 
\end{tcolorbox}
\textbf{Solution.}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Define the discounted price process. Show that in the Black-Scholes model, the discounted price-process is a martingale if and only if $\mu = r$.
\end{tcolorbox}
\textbf{Solution.}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $S_1$ and $S_2$ be two non-dividend paying stocks that evolving according to geometric Brownian motion. Find a stochastic differential for $S_1 + S_2$ when $S_1$ and $S_2$ are independent, and when they share correlation equivalent $\rho$.  \cite{Hull}
\end{tcolorbox}
\textbf{Solution.}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} (Black-Scholes Via the EMM). Using an equivalent martingale measure and assumptions of the Black-Scholes model, show that the price of a Call option struck at $K$ satisfies
$$C_t = S_t\Phi(h_t) - Ke^{-r(T-t)}\Phi(h_t-\sigma\sqrt{T-t})$$
where 
$$h_t = \frac{\ln\frac{S_t}{K} + (r+\frac{1}{2}\sigma^2)(T-t)}{\sigma\sqrt{T-t}}$$
\end{tcolorbox}
\textbf{Proof.}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} (Black-Scholes Via the PDE). 
\end{tcolorbox}
\textbf{Proof.}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Suppose $S_t$ follows geometric brownian motion and let $G$ be a function of $S_t$ and $t$. Show that when the expected return of $S_t$ increases by $\lambda \sigma _S$ the expected return of $G$ grows by $\lambda \sigma _G$. \cite{Hull}
\end{tcolorbox}
\textbf{Proof.} To begin, note that $dS_t = \mu S_t dt + \sigma_S S_tdB_t$. We now apply Itô's lemma so that 
\begin{align*}
   dG &= \frac{\partial G}{\partial S_t}dS_t + \frac{\partial G}{\partial t}dt + \frac{\partial^2 G}{\partial S_t^2}\sigma^2_s S_t^2dt  \\
   &= \frac{\partial G}{\partial S_t}(\mu S_t dt + \sigma_sS_tdB_t) + \frac{\partial G}{\partial t}dt + \frac{\partial^2 G}{\partial S_t^2}\sigma^2_s S_t^2dt \\
   &= \left(\frac{\partial G}{\partial S_t}\mu S_t + \frac{\partial G}{\partial t} + \frac{\partial^2 G}{\partial S_t^2}\sigma_S^2S_t^2\right)dt + \frac{\partial G}{\partial S_t}\sigma_S S_t dB_t
\end{align*}
From here one sees that 
\begin{align*}
     & \sigma_G = \frac{\partial G}{\partial S_t}\sigma_S S_t \\
    & \mu_G  = \frac{\partial G}{\partial S_t}\mu S_t + \frac{\partial G}{\partial t} + \frac{\partial^2 G}{\partial S_t^2}\sigma_S^2S_t^2
\end{align*}
The \textit{expected return of $S_t$} is controlled entirely through $\mu$. So to increase the expected rate of return by $\lambda\sigma_S$ we update the mean to $\mu + \lambda\sigma_S$. Thus, $\mu_G$ is increased by
$$\frac{\partial G}{\partial S_t}\lambda\sigma_S S_t = \lambda\sigma_G$$
as claimed. $\square$
 
\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Derive the price evolution of a Zero Coupon Bond paying \$1 at time $T$, given the yield $X_t$ on the bond evolves according to $dX_t = \alpha(X_t-X_0)dX_t + \sigma X_tdB_t$. \cite{Hull}
\end{tcolorbox}
\textbf{Proof}. The price of the bond at time $t\leq T$ is $P_t = e^{-X_t(T-t)}$. The trick is to now apply Itô's lemma to $f(s) = e^{-s(T-t)}$.
\newpage
\bibliographystyle{plain} 
\bibliography{references}  % Assumes a references.bib file

\end{document}
