\documentclass{article}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{natbib} 
\usepackage{tcolorbox} 
\usepackage{amsmath}
\usepackage{amssymb}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }


\begin{document}

% Cover Page
\begin{titlepage}
    \centering
    {\Huge \textbf{Stochastic Calculus}}\\[1.5cm] % Title
    \textbf{Author:} Sean Conlon\\[1cm] % Replace 'Your Name' with your actual name
    \textbf{Date:} \today\\[3cm] % Automatically inserts today's date
    
    \section*{About}
    This document is a collection of exercises and solutions for problems in Stochastic Calculus. The exercises are primarily sourced from the University of Melbourne's \texttt{MAST90059 Stochastic Calculus} subject, though some are also taken from other relevant textbooks. Any mistakes are entirely my own. Please email me at \texttt{sc.[first].[last]@gmail.com} for any questions or amendments.\\[2cm]

    % Table of Contents
    \tableofcontents
    
\end{titlepage}

% Example sections (these will appear in the table of contents)
\newpage
\section{Ordinary Calculus}

\newpage
\section{Probability \& Measure Theory Fundamentals}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} For a random variable $X\geq0$ show that $\mathbb{E}[X] = \int_{0}^{\infty}P(X\geq x)dx$
\end{tcolorbox}
\textbf{Proof.} For $X\geq0$ We have 
\begin{align*}
    E[X] = \int_{0}^{\infty}xf(x)dx &= \int_{0}^{\infty} x dF(x) \\
    &= \int_{0}^{\infty}\int_{0}^{x}dtdF(x)
\end{align*}
We now wish to apply Fubini's theorem to change the order of integration. Notice that the region of integration runs over $0\leq t\leq x<\infty$ so after applying Fubini's theorem
\begin{align*}
    \int_{0}^{\infty}\int_{0}^{x}dtdF(x) &= \int_{0}^{\infty}\int_{x}^{\infty}dF(x)dt \\
    &=\int_{0}^{\infty}(1 - F(t))dt \\
    &= \int_{0}^{\infty} P(X\geq t)dt \hspace{10mm}
    \square\\
\end{align*}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} (A common misconception) Show that two normally distributed and uncorrelated random variables are not necessarily independent. 
\end{tcolorbox}
\textbf{Proof.} Let $Z_1,Z_2\stackrel{iid}{\sim}N(0,1)$ and define $X = Z_1 + Z_2$ and  $Y = Z_1 - Z_2$. One verifies 
\begin{align*}
    \text{Cov}(X_, Y) = E[XY] &= E[Z_1^2 -Z_2^2] \\
    &= 0.
\end{align*}
So it is clear that $X$ and $Y$ are uncorrelated. We now wish to show that they are not independent. To derive the joint distribution of $X,Y$ we first consider the joint distribution of $Z_1, Z_2$ then apply a change of variables. As $Z_i$ are \textit{iid} their joint distribution is simply the product of the marginal distributions. That is
$$f_{Z_1, Z_2}(z_1, z_2) = \frac{1}{2\pi}e^{-\frac{1}{2}(z_1^2 + z_2^2)}.$$
Given the definitions of $X$ and $Y$ we now apply the change of variables 
$$Z_1 = \frac{X+Y}{2} \hspace{5mm} Z_2 = \frac{X-Y}{2}$$
One now considers 
\begin{align*}
    f_{X,Y}(x, y) &= f_{Z_1, Z_2}\left(\frac{x+y}{2}, \frac{x-y}{2}\right) \\
    &= \frac{1}{2\pi}e^{-\frac{1}{4}(x^2 + y^2)}.
\end{align*}
To verify that they are not independent, we now check the marginal distributions. 


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Suppose $(X,Y)$ has the following joint distribution
$$f_{X,Y}(x,y) = \frac{1}{2\pi\sqrt{1-\rho^2}}\exp\left(\frac{x^2 - 2\rho xy + y^2}{2(1-\rho^2)}\right)\quad x\in\mathbb{R},y\in\mathbb{R}$$
Find and interperate the Marginal distributions. [UniMelb]
\end{tcolorbox}
\textbf{Solution.} The critical observation is to factor the numerator in the exponent. We rewrite $$x^2 -2\rho xy + y^2 = (x-\rho y)^2 + (1-\rho^2)y^2$$
We can now rewrite the density function as
$$f_{X,Y}(x,y) = \frac{1}{2\pi\sqrt{1-\rho^2}}\exp\left(\frac{(x-\rho y)^2}{2(1-\rho^2)} - \frac{y^2}{2}\right)$$
We now integrate
\begin{align*}
    f_Y(y) &= \int_{X} f_{X,Y}dx \\
    &= \frac{1}{\sqrt{2\pi}}e^{-y^2/2}\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi(1-\rho^2)}}\exp\left(\frac{(x-\rho y)^2}{2(1-\rho^2)}\right)dx \\
    &\stackrel{d}{=} N(0,1)
\end{align*} 

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $T$ be a stopping time of the filtration $\mathcal{F}_t$. Define 
$$\mathcal{F}_T := \{A\in\mathcal{F}_t : A\cap\{T\leq t\}\in\mathcal{F}_t\}$$
Prove that $\mathcal{F}_T$ is a $\sigma$-algebra and $T$ is $\mathcal{F}_T$-measurable.
\end{tcolorbox}
\textbf{Proof.} To see that $\mathcal{F}_T$ is a $\sigma$-Algebra: 
\begin{enumerate}
    \item (Contains $\Omega$). Since $\Omega\in\mathcal{F}_t$ be definition and $\Omega \cap\{T\leq t\} = \{T\leq t\}\in\mathcal{F}_t$ we conclude that $\Omega\in\mathcal{F}_T$. 
    \item (Closure Under Compliments). Suppose $A\in\mathcal{F}_T$. Then $A^c\cap\{T\leq t\} = \{T\leq t\}\backslash A $
\end{enumerate}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} $X\sim N(\mu,\sigma^2)$ find the distribution function of $Y = |X|.$
\end{tcolorbox}
\textbf{Proof.}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Show that the characteristic function of a $N(\mu, \sigma^2)$ random variable is 
$$\varphi_X(t) = \exp(i\mu t - \sigma^2t^2/2)$$
\end{tcolorbox}
\textbf{Proof.}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} (Limits of Gaussian Random Variables are Gaussian) Let $\{Y_n:n\geq 0\}$ be a sequence of Gaussian Random variables such that $Y_n\stackrel{d}{\to} Y$. Show that $Y$ is either degenerate or Gaussian. 
\end{tcolorbox}
\textbf{Proof.}

\newpage
\section{Brownian Motion}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Show that $2B_{t} - B_s$ is not independent of $B_s$. \cite{Fima}
\end{tcolorbox}
\textit{Proof.} There are many ways to prove this, though our approach will show that $E[(2B_t-B_s)B_s]\neq E[2B_t-B_s] E[B_s]$ to include that the two terms are not independent. We have: 
\begin{align*}
    E[(2B_t-B_s)B_s] &= E[2B_tB_s - B_s^2] \\
    &= 2E[B_t B_s] - s
\end{align*}
Without loss of generality, we assume that $s<t$. We then apply the usual identity $B_t = B_s + (B_t - B_s)$ to the first term. This yields
\begin{align*}
    E[B_t B_s] &= E[(B_s + (B_t - B_s))B_s] \\
    &=E[B_s^2] + E[B_s(B_t - B_s)] \\
    &= s
\end{align*}
The last term is obtained by noting that $B_s \perp B_{t} - B_s$ (Independant Increments). We then substitute this into our orignal equation to see that 
$$E[(2B_t-B_s)B_s] = s \neq E[2B_t-B_s]E[B_s] \hspace{10mm} \Box$$

\vspace{2mm}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Find $P(B_t\leq0\text{ for } t=0,1,2)$ \cite{Fima}
\end{tcolorbox}
\textbf{Solution:} We assume standard Brownian Motion, so $B_0=0$. The problem is now to find $P(B_1\leq0, B_2\leq0)$.


\vspace{2mm}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $B_t$ denote a standard Brownian Motion. Which of the following are also Brownian Motion? \cite{Fima}
\begin{enumerate}
    \item $X_t = -B_t$
    \item $X_t = B_{2t} - B_t$
\end{enumerate} 
\end{tcolorbox}

\textbf{Solution:}\\
1) $X_t = -B_t$ defines a Brownian Motion. We can verify directly from the definition: 
\begin{itemize}
    \item (Indep. Increments). $X_t - X_s = -(B_t - B_s)$ which is independent of $X_u$ for $0\leq u <s$ from the definition of Brownian Motion. 
    \item (Normal Increments) $X_t - X_s = -(B_t - B_s) \sim N(0, t-s)$
    \item (Continuous Paths) $X_t$ is clearly continuous.
\end{itemize}
2) $X_t = t B_{1/t}$ also defines a Brownian Motion.

\vspace{2mm}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Find $\mathbb{P}(\int_0^1B_tdt > 2/\sqrt{3})$ \cite{Fima}
\end{tcolorbox}
\textbf{Solution:} $\mathbb{P}(\int_0^1B_tdt > 2/\sqrt{3}) = 1-\mathbb{P}(\int_0^1B_tdt \leq 2/\sqrt{3})$. From here, we see that if we can identify the distribution of $\int_{0}^{1}B_tdt$ then we we are pretty much done. To so so, we can approximate the integral by partitioning the interval into $0<t_1<t_2<\dots<1$, then
$$\int_{0}^{1}B_t dt \approx \sum_{i=1}^{n}B_{t_i}\Delta_{t_i} = \sum_{i=1}^{n}B_{t_i}(t_{i}-t_{_{i-t}})$$
We recall that $B_t$ is a Gaussian Process, and thus $\sum_iB_{t_i}\Delta_{t_i}$ is a linear combination of Gaussian processes and is therefore also a Gaussian Process. Finally we note that under taking the limit 
$$\lim_{\Delta\to0}\sum_{i=1}B_{t_i}\Delta_{t_i} = \int_{0}^{1}B_t dt$$
the limiting process is also Gaussian with equal mean. So far, we have $\int_{0}^{1}B_tdt\sim N(0, \sigma^2)$. To find the variance we consider
\begin{align*}
    \text{Var}\left(\int_{0}^{1}B_tdt\right) &= \text{Cov}\left(\int_{0}^{1}B_tdt\int_{0}^{1}B_sds\right) \\
    &= \mathbb{E}\left(\int_{0}^{1}B_tdt\int_{0}^{1}B_sds\right) \\
    &=\int_{0}^{1}\int_{0}^{1}\mathbb{E}[B_t B_s]dtds \hspace{10mm} \text{(Fubini)}\\
    &= \int_{0}^{1}\int_{0}^{1}\min(s, t)dtds
\end{align*}
To examine the integral, notice that the integral runs from $[0,t]$ when $s<t$ and $[0,s]$ when $t<s$. Drawing this out shows that they occupy the same area. Thus, 
\begin{align*}
    \int_{0}^{1}\int_{0}^{1}\min(s, t)dtds &= \int_{0}^{1}\int_{0}^{t}sdsdt + \int_{0}^{1}\int_{0}^{s}tdtds \\
    &= 2\int_{0}^{1}\int_{0}^{t}sdsdt \\
    &= \frac{1}{3}
\end{align*}
We conclude $\int_{0}^{1}B_tdt\sim N(0, 1/3)$.

\vspace{2mm}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Find the distribution of $B_t + B_s$ where $s < t$.
\end{tcolorbox}
\textbf{Solution.} It is important to note that $B_t$ is not independent of $B_s$ so we can't simply sum them as if they were independent normal variables. Instead 
\begin{align*}
    B_t + B_s = B_s + (B_t - B_s) + B_s &= 2B_s + (B_t-B_s) \\
    &\stackrel{d}{=}N(0, 4s) + N(0, t-s) \\
    &\stackrel{d}{=}N(0, t + 3s)
\end{align*}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $B_t$ and $W_t$ be two independant Brownian Motions. Show that the process $X_t = \frac{B_t + W_t}{\sqrt{2}}$ also defines a Brownian Motion. Find the correlation between $X_t$ and $B_t$. 
\end{tcolorbox}
\textbf{Solution.} It is straightforward to verify that $X_t$ is a Brownian Motion. To find the correlation recall that 
$$\text{Corr}(X_t, B_t) = \frac{\text{Cov}(X_t, B_t)}{\sqrt{\text{Var($X_t$)}}\sqrt{\text{Var($B_t$)}}} = \frac{\text{Cov}(X_t, B_t)}{t}$$
To find the covariance of the two processes
\begin{align*}
    \text{Cov}(X_t ,B_t) &= \frac{1}{\sqrt{2}}(E[B_t^2] + E[B_tW_t]) \\
    &= \frac{t}{\sqrt{2}}
\end{align*}
From here, we conclude Corr$(X_t, B_t) = 1/\sqrt{2}$

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Prove that $B_t$ is nowhere differentiable a.s. [UniMelb]  
\end{tcolorbox}
\textbf{Proof.} We begin by noting 
\begin{align*}
    \frac{B_{t+\delta} - B_t}{\delta} \stackrel{d}{=} \frac{\sqrt{\delta} Z}{\delta} \stackrel{d}{=} \frac{Z}{\sqrt{\delta}}
\end{align*}
It is now clear that for any $K > 0$ fixed
\begin{align*}
    \mathbb{P}\left(\left|\frac{Z}{\sqrt{\delta}}\right| > K \right) =1 \quad \text{as } \quad \delta\to0. 
\end{align*}
Thus with probability 1, we conclude that $B_t$ is nowhere differentiable. $\square$

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} (Fractal Scaling Property of Brownian Motion) Show that for any fixed $a>0$  the process $X_t = aB_{t/a^2}$ is itself a Brownian Motion.  
\end{tcolorbox}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} State and Prove the Markov Property for Brownian Motion.
\end{tcolorbox}
\textbf{Solution.} The a process satisfies the Markov Property (is Markovian) if the future trajectory of the process is independent of its past. That is, the future evolution of the process depends only on its current position, and no further history. Mathematically, 
$$P(X_{t+s} \leq y| \mathcal{F}_t) = P(X_{t+s} \leq y| X_t=x)$$
To see that Brownian Motion satisfies the Markov Property let $s <t$ and consider 
\begin{align*}
    P(B_t \leq y| \mathcal{F}_s) &= P(B_s + (B_t-B_s) \leq y|\mathcal{F}_s) \\
    &= P(B_t \leq y| B_s =x) \\
\end{align*}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} (Exit Time and Probability of Brownian Motion) 
\end{tcolorbox}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $s < t$, find the distribution of $B_s | B_t = b$.
\end{tcolorbox}
\textbf{Solution.} The \textit{trick} to this problem is to recognize that $(B_s, B_t)$ is multivariate normal then apply the \href{https://online.stat.psu.edu/stat505/lesson/6/6.1}{formulae for conditional distributions of multivariate Gaussians}. That is: 
\begin{align*}
    & E[B_s | B_t=b] = \frac{s}{t}b \\
    &\text{Var}(B_s | B_t=b) = s-\frac{s^2}{t} = \frac{s(t-s)}{t}
\end{align*}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Show that for any $0\leq a < b<\infty$ Brownian Motion is non-monotone on the interval $[a,b]$ with probability 1.  
\end{tcolorbox}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} By considering $M_t := \max_{0\leq s \leq t} B_s$ and $T_a$ be the first time that $B_t$ hits $a > 0$. Prove that $P(\tau_x\leq t) = P(|B_t|\geq x)$  
\end{tcolorbox}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Show that $|B_t| \stackrel{d}{=} \max_{0\leq s\leq t} B_s$.
\end{tcolorbox}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Prove the law of large numbers for Brownian Motion
$$\lim_{t\to\infty}\frac{B_t}{t} = 0 \text{ a.s.}$$
\end{tcolorbox}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Prove the law of the iterated logarithm for Brownian Motion:
\end{tcolorbox}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Argue that the process $X_t := \int_0^t B_sds$ exists. Show that it is a Gaussian Process, state its mean, variance and Covariance Function.
\end{tcolorbox}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Show that the zeroes of Brownian Motion for an uncountable set which has zero measure $\mathbb{P}$ - almost surely.
\end{tcolorbox}
\textbf{Proof.}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Show that for any $x\geq 0$ 
$$\mathbb{P}(\max_{s\in[0,t]} B_t\geq x) = 2\left(1- \Phi\left(\frac{x}{\sqrt{t}}\right)\right).$$
\end{tcolorbox}
\textbf{Proof.} Define $\tau_x := \inf_{t\geq0}\{B_t=x\}$ and follow the usual steps for deriving the maximum of Brownian Motion: 

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Show that for any $x<0$ 
$$\mathbb{P}(\min_{s\in[0,t]} B_t\leq x) = 2\mathbb{P}(B_t\leq x).$$
\end{tcolorbox}
\textbf{Proof.} 

\newpage
\section{Brownian Motion Calculus}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $X_t=-1$ on $0\leq t\leq1$, $X_t=1$ on $1 < t\leq2$ and $X_t=2$ on $2 < t\leq3$. Find the distribution of $\int_0^3X_sdB_s$. \cite{Fima}
\end{tcolorbox}
\textbf{Solution.} This is an Itô integral for a simple, nonstochastic process. By Definition
\begin{align*}
    \int_0^3 X_sdB_s &:= -(B_1-B_0) + (B_2-B_1)+2(B_3-B_2) \\ 
    &= 2B_3 -B_2-2B_1 \\
\end{align*}
We now the distribution will be gaussian with 0 mean. To find the variance we recall Cov$(B_t, B_s) = \min(t,s)$ and so
\begin{align*}
    \text{Var}(2B_3 -B_2-2B_1) &= \text{Var}(2B_3) + \text{Var}(B_2) + \text{Var}(2B_1) - 2\text{Cov}(B_3, B_2)-4\text{Cov}(B_3, B_2) +2\text{Cov}(B_2,B_1) \\
    &= 12 + 2 + 4-4-8+2 \\
    &= 6
\end{align*}
Hence we conclude $\int_0^3X_sdB_s\sim N(0,6).$


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} By finding a suitable sequence of Simple Adapted Processes, find $\int_0^T B_s dB_s$. \cite{Fima}
\end{tcolorbox}
\textbf{Solution.}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Find the values of $\alpha$ for which the process $Y_t = \int_0^t(t-s)^{-\alpha}dB_s$ is defined.
\end{tcolorbox}
\textbf{Solution}. Recall that $\int_0^t X_sdB_s$ is well-defined so long as $\int_0^t E[X_s^2]ds<\infty$. Applying this to $Y_t$ we require $\int_0^t(t-s)^{-2\alpha}ds <\infty$. Make the substitution $u = (t-s)$ then\footnote{Notice that $s$ ranges from $[0,t]$ in the original integrand, so when we make the $u$ substitution under the original end-points $u = t-s$ now varies from $t$ down to $0$. We then switch the order of these bounds which multiples through by $-1$.}
\begin{align*}
    \int_0^t(t-s)^{-2\alpha}ds &= \int_t^0 u^{-2\alpha}(-du) \\
    &= \int_0^tu^{-2\alpha}du
\end{align*}
From here, one clearly sees this integral is finite provided $\alpha < 1/2.$ $\square$

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Find the Stochastic Differential $d(Y_t / Z_t)$ where $Y_t = tB_t$ and $Z_t=e^{B_t}$ using Itô's formula. \cite{Fima}
\end{tcolorbox}
\textbf{Solution.} For exercises like this, the approach is: 
\begin{enumerate}
    \item Identify a suitable function $f$ to apply Itô's formula to. 
    \item Compute the partial derivatives of $f$. 
    \item Compute the differentials $dZ_t$ and $dY_t$. 
    \item Substitute everything in and cross cancel. 
\end{enumerate}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Derive the Stochastic Calculus equivalent of the Product Rule by applying Itô's lemma. 
\end{tcolorbox}
\textbf{Solution.} Consider the function $f(x,y) = xy$. One verifies it has the partial derivatives $f_x(x,y)=y$, $f_y(x,y)=x$, $f_{x,y}(x,y)=1$ and vanishing second-order derivatives. Applying Ito's formula for multi-variate functions one finds
$$dX_tY_t = Y_tdX_t + X_tdY_t + \sigma_X(t)\sigma_Y(t)dt.$$


\newpage
\section{Stochastic Differential Equations}


\newpage
\section{Applications to Finance}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $S_1$ and $S_2$ be two non-dividend paying stocks that evolving according to geometric Brownian motion. Find a stochastic differential for $S_1 + S_2$ when $S_1$ and $S_2$ are independent, and when they share correlation equivalent $\rho$.  \cite{Hull}
\end{tcolorbox}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Suppose $S_t$ follows geometric brownian motion and let $G$ be a function of $S_t$ and $t$. Show that when the expected return of $S_t$ increases by $\lambda \sigma _S$ the expected return of $G$ grows by $\lambda \sigma _G$. \cite{Hull}
\end{tcolorbox}
\textbf{Proof.} To begin, note that $dS_t = \mu S_t dt + \sigma_S S_tdB_t$. We now apply Itô's lemma so that 
\begin{align*}
   dG &= \frac{\partial G}{\partial S_t}dS_t + \frac{\partial G}{\partial t}dt + \frac{\partial^2 G}{\partial S_t^2}\sigma^2_s S_t^2dt  \\
   &= \frac{\partial G}{\partial S_t}(\mu S_t dt + \sigma_sS_tdB_t) + \frac{\partial G}{\partial t}dt + \frac{\partial^2 G}{\partial S_t^2}\sigma^2_s S_t^2dt \\
   &= \left(\frac{\partial G}{\partial S_t}\mu S_t + \frac{\partial G}{\partial t} + \frac{\partial^2 G}{\partial S_t^2}\sigma_S^2S_t^2\right)dt + \frac{\partial G}{\partial S_t}\sigma_S S_t dB_t
\end{align*}
From here one sees that 
\begin{align*}
     & \sigma_G = \frac{\partial G}{\partial S_t}\sigma_S S_t \\
    & \mu_G  = \frac{\partial G}{\partial S_t}\mu S_t + \frac{\partial G}{\partial t} + \frac{\partial^2 G}{\partial S_t^2}\sigma_S^2S_t^2
\end{align*}
The \textit{expected return of $S_t$} is controlled entirely through $\mu$. So to increase the expected rate of return by $\lambda\sigma_S$ we update the mean to $\mu + \lambda\sigma_S$. Thus, $\mu_G$ is increased by
$$\frac{\partial G}{\partial S_t}\lambda\sigma_S S_t = \lambda\sigma_G$$
as claimed. $\square$
 
\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Derive the price evolution of a Zero Coupon Bond paying \$1 at time $T$, given the yield $X_t$ on the bond evolves according to $dX_t = \alpha(X_t-X_0)dX_t + \sigma X_tdB_t$. \cite{Hull}
\end{tcolorbox}
\textbf{Proof}. The price of the bond at time $t\leq T$ is $P_t = e^{-X_t(T-t)}$. The trick is to now apply Itô's lemma to $f(s) = e^{-s(T-t)}$.
\newpage
\bibliographystyle{plain} 
\bibliography{references}  % Assumes a references.bib file

\end{document}
