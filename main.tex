\documentclass{article}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{natbib} 
\usepackage{tcolorbox} 
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{todonotes}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }


\begin{document}

% Cover Page
\begin{titlepage}
    \centering
    {\Huge \textbf{Stochastic Calculus}}\\[1.5cm] % Title
    \textbf{Author:} Sean Conlon\\[1cm] % Replace 'Your Name' with your actual name
    \textbf{Date:} \today\\[3cm] % Automatically inserts today's date
    
    \section*{About}
    This document is a collection of exercises and solutions for problems in Stochastic Calculus. The exercises are primarily sourced from the University of Melbourne's \texttt{MAST90059 Stochastic Calculus} subject, though some are also taken from other relevant textbooks. Any mistakes are entirely my own. Please email me at \texttt{sc.[first].[last]@gmail.com} for any questions or amendments.\\[2cm]

    % Table of Contents
    \tableofcontents
    
\end{titlepage}

\newpage
\section{Ordinary Calculus}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Show that if $f$ has finite variation on an $[a,b]$ then $f$ must be bounded on $[a,b]$
\end{tcolorbox}
\textbf{Proof.} Let $V^f([a,b]) = V<\infty$. Let $x\in(a,b)$ then since $V^f$ is increasing we have 
\begin{align*}
    |f(x)-f(b)| \leq V^f([x,b]) \leq V^f([a,b]) < V
\end{align*}
By the triangle inequality we therefore have $|f(x)|\leq V+|f(b)|$ which implies $f$ is bounded on $[a,b].$ $\square$

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Show that if $f$ is monotone on $[a,b]$ then $V^f([a,b]) = |f(b)-f(a)|$
\end{tcolorbox}
\textbf{Proof.} Suppose $f$ is increasing. Then $f(t_i) < f(t_{i+1})$ for all possible partitions $\Pi$. We therefore have
\begin{align*}
    V^f([a,b]) = \sup_{\Pi}\sum_{{i=0}}^{n}|f(t_{i+1}) - f(t_i)| &= \sup_{\Pi}\sum_{{i=0}}^{n}f(t_{i+1}) - f(t_i) \\
    &= f(b) - f(a)
\end{align*}
Analogously for $f\searrow$ we have $f(t_i) > f(t_{i+1})$ so for any possible partition we have
\begin{align*}
    V^f([a,b]) = \sup_{\Pi}\sum_{{i=0}}^{n}|f(t_{i+1}) - f(t_i)| = f(a) - f(b)
\end{align*}
Combine both casses and we have the claim $\square$.

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $g(x) = \sin x + \sqrt{3}\cos x$ on $0\leq x \leq \pi$. Find the variance of $g$ and a Jordan Decomposition of $g$.
\end{tcolorbox}
\textbf{Proof.}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Five a function $h$ on [0,1] and sequence of partitions $0 = t_0^n < ... < t_n^n=1$ with $\max |t_i^n - t_{i-1}^n| \to 0$ as $n \to \infty$ such that $$V^h([0,1]) > \lim_{n\to\infty} \sum_{i=1}^{n}|h(t_{i}) - h(t_{i-1})|$$
\end{tcolorbox}
\textbf{Solution.}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Show that a function of finite variation can only have countably many jumps
\end{tcolorbox}
\textbf{Proof.} Let $g$ be a function of finite variation. Recall that a \textit{jump} is a point $t\in[a,b]$ such that $g(t-) \neq g(t+)$. Let $\mathcal{D}$ be the set of all points of jump points on $[a,b]$. Then the total variation on $[a,b]$ must be greater than the variation at each jump point. That is, 
$$V^g([a,b])\geq \sum_{t\in\mathcal{D}}|g(t) - g(t-)|$$
Now, if $\mathcal{D}$ is uncountable, then the right hand side becomes an uncountable sum of positive values, diverging to infinity, and thus we would have $V^g([a,b])=\infty$ which contradicts our assumption of $g$ having finite variation. $\square$





\newpage
\section{Probability \& Measure Theory Fundamentals}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} For a random variable $X\geq0$ show that $\mathbb{E}[X] = \int_{0}^{\infty}P(X\geq x)dx$
\end{tcolorbox}
\textbf{Proof.} For $X\geq0$ We have 
\begin{align*}
    E[X] = \int_{0}^{\infty}xf(x)dx &= \int_{0}^{\infty} x dF(x) \\
    &= \int_{0}^{\infty}\int_{0}^{x}dtdF(x)
\end{align*}
We now wish to apply Fubini's theorem to change the order of integration. Notice that the region of integration runs over $0\leq t\leq x<\infty$ so after applying Fubini's theorem
\begin{align*}
    \int_{0}^{\infty}\int_{0}^{x}dtdF(x) &= \int_{0}^{\infty}\int_{x}^{\infty}dF(x)dt \\
    &=\int_{0}^{\infty}(1 - F(t))dt \\
    &= \int_{0}^{\infty} P(X\geq t)dt \hspace{10mm}
    \square\\
\end{align*}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} (A common misconception) Show that two normally distributed and uncorrelated random variables are not necessarily independent. 
\end{tcolorbox}
\textbf{Proof.} Let $Z_1,Z_2\stackrel{iid}{\sim}N(0,1)$ and define $X = Z_1 + Z_2$ and  $Y = Z_1 - Z_2$. One verifies 
\begin{align*}
    \text{Cov}(X_, Y) = E[XY] &= E[Z_1^2 -Z_2^2] \\
    &= 0.
\end{align*}
So it is clear that $X$ and $Y$ are uncorrelated. We now wish to show that they are not independent. To derive the joint distribution of $X,Y$ we first consider the joint distribution of $Z_1, Z_2$ then apply a change of variables. As $Z_i$ are \textit{iid} their joint distribution is simply the product of the marginal distributions. That is
$$f_{Z_1, Z_2}(z_1, z_2) = \frac{1}{2\pi}e^{-\frac{1}{2}(z_1^2 + z_2^2)}.$$
Given the definitions of $X$ and $Y$ we now apply the change of variables 
$$Z_1 = \frac{X+Y}{2} \hspace{5mm} Z_2 = \frac{X-Y}{2}$$
One now considers 
\begin{align*}
    f_{X,Y}(x, y) &= f_{Z_1, Z_2}\left(\frac{x+y}{2}, \frac{x-y}{2}\right) \\
    &= \frac{1}{2\pi}e^{-\frac{1}{4}(x^2 + y^2)}.
\end{align*}
To verify that they are not independent, we now check the marginal distributions. 


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Suppose $(X,Y)$ has the following joint distribution
$$f_{X,Y}(x,y) = \frac{1}{2\pi\sqrt{1-\rho^2}}\exp\left(\frac{x^2 - 2\rho xy + y^2}{2(1-\rho^2)}\right)\quad x\in\mathbb{R},y\in\mathbb{R}$$
Find and interperate the Marginal distributions. [UniMelb]
\end{tcolorbox}
\textbf{Solution.} The critical observation is to factor the numerator in the exponent. We rewrite $$x^2 -2\rho xy + y^2 = (x-\rho y)^2 + (1-\rho^2)y^2$$
We can now rewrite the density function as
$$f_{X,Y}(x,y) = \frac{1}{2\pi\sqrt{1-\rho^2}}\exp\left(\frac{(x-\rho y)^2}{2(1-\rho^2)} - \frac{y^2}{2}\right)$$
We now integrate
\begin{align*}
    f_Y(y) &= \int_{X} f_{X,Y}dx \\
    &= \frac{1}{\sqrt{2\pi}}e^{-y^2/2}\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi(1-\rho^2)}}\exp\left(\frac{(x-\rho y)^2}{2(1-\rho^2)}\right)dx \\
    &\stackrel{d}{=} N(0,1)
\end{align*} 

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $T$ be a stopping time of the filtration $\mathcal{F}_t$. Define 
$$\mathcal{F}_T := \{A\in\mathcal{F}_t : A\cap\{T\leq t\}\in\mathcal{F}_t\}$$
Prove that $\mathcal{F}_T$ is a $\sigma$-algebra and $T$ is $\mathcal{F}_T$-measurable.
\end{tcolorbox}
\textbf{Proof.} To see that $\mathcal{F}_T$ is a $\sigma$-Algebra: 
\begin{enumerate}
    \item (Contains $\Omega$). Since $\Omega\in\mathcal{F}_t$ be definition and $\Omega \cap\{T\leq t\} = \{T\leq t\}\in\mathcal{F}_t$ we conclude that $\Omega\in\mathcal{F}_T$. 
    \item (Closure Under Compliments). Suppose $A\in\mathcal{F}_T$. Then $A^c\cap\{T\leq t\} = \{T\leq t\}\backslash A $
\end{enumerate}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} $X\sim N(\mu,\sigma^2)$ find the distribution function of $Y = |X|.$
\end{tcolorbox}
\textbf{Proof.}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Show that the characteristic function of a $N(\mu, \sigma^2)$ random variable is 
$$\varphi_X(t) = \exp(i\mu t - \sigma^2t^2/2)$$
\end{tcolorbox}
\textbf{Proof.}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} (Limits of Gaussian Random Variables are Gaussian) Let $\{Y_n:n\geq 0\}$ be a sequence of Gaussian Random variables such that $Y_n\stackrel{d}{\to} Y$. Show that $Y$ is either degenerate or Gaussian. 
\end{tcolorbox}
\textbf{Proof.}

\newpage
\section{Brownian Motion}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Show that $2B_{t} - B_s$ is not independent of $B_s$. \cite{Fima}
\end{tcolorbox}
\textit{Proof.} There are many ways to prove this, though our approach will show that $E[(2B_t-B_s)B_s]\neq E[2B_t-B_s] E[B_s]$ to include that the two terms are not independent. We have: 
\begin{align*}
    E[(2B_t-B_s)B_s] &= E[2B_tB_s - B_s^2] \\
    &= 2E[B_t B_s] - s
\end{align*}
Without loss of generality, we assume that $s<t$. We then apply the usual identity $B_t = B_s + (B_t - B_s)$ to the first term. This yields
\begin{align*}
    E[B_t B_s] &= E[(B_s + (B_t - B_s))B_s] \\
    &=E[B_s^2] + E[B_s(B_t - B_s)] \\
    &= s
\end{align*}
The last term is obtained by noting that $B_s \perp B_{t} - B_s$ (Independant Increments). We then substitute this into our orignal equation to see that 
$$E[(2B_t-B_s)B_s] = s \neq E[2B_t-B_s]E[B_s] \hspace{10mm} \Box$$

\vspace{2mm}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Find $P(B_t\leq0\text{ for } t=0,1,2)$ \cite{Fima}
\end{tcolorbox}
\textbf{Solution:} We assume standard Brownian Motion, so $B_0=0$. The problem is now to find $P(B_1\leq0, B_2\leq0)$.


\vspace{2mm}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $B_t$ denote a standard Brownian Motion. Which of the following are also Brownian Motion? \cite{Fima}
\begin{enumerate}
    \item $X_t = -B_t$
    \item $X_t = B_{2t} - B_t$
\end{enumerate} 
\end{tcolorbox}

\textbf{Solution:}\\
1) $X_t = -B_t$ defines a Brownian Motion. We can verify directly from the definition: 
\begin{itemize}
    \item (Indep. Increments). $X_t - X_s = -(B_t - B_s)$ which is independent of $X_u$ for $0\leq u <s$ from the definition of Brownian Motion. 
    \item (Normal Increments) $X_t - X_s = -(B_t - B_s) \sim N(0, t-s)$
    \item (Continuous Paths) $X_t$ is clearly continuous.
\end{itemize}
2) $X_t = t B_{1/t}$ also defines a Brownian Motion.

\vspace{2mm}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Find $\mathbb{P}(\int_0^1B_tdt > 2/\sqrt{3})$ \cite{Fima}
\end{tcolorbox}
\textbf{Solution:} $\mathbb{P}(\int_0^1B_tdt > 2/\sqrt{3}) = 1-\mathbb{P}(\int_0^1B_tdt \leq 2/\sqrt{3})$. From here, we see that if we can identify the distribution of $\int_{0}^{1}B_tdt$ then we we are pretty much done. To so so, we can approximate the integral by partitioning the interval into $0<t_1<t_2<\dots<1$, then
$$\int_{0}^{1}B_t dt \approx \sum_{i=1}^{n}B_{t_i}\Delta_{t_i} = \sum_{i=1}^{n}B_{t_i}(t_{i}-t_{_{i-t}})$$
We recall that $B_t$ is a Gaussian Process, and thus $\sum_iB_{t_i}\Delta_{t_i}$ is a linear combination of Gaussian processes and is therefore also a Gaussian Process. Finally we note that under taking the limit 
$$\lim_{\Delta\to0}\sum_{i=1}B_{t_i}\Delta_{t_i} = \int_{0}^{1}B_t dt$$
the limiting process is also Gaussian with equal mean. So far, we have $\int_{0}^{1}B_tdt\sim N(0, \sigma^2)$. To find the variance we consider
\begin{align*}
    \text{Var}\left(\int_{0}^{1}B_tdt\right) &= \text{Cov}\left(\int_{0}^{1}B_tdt\int_{0}^{1}B_sds\right) \\
    &= \mathbb{E}\left(\int_{0}^{1}B_tdt\int_{0}^{1}B_sds\right) \\
    &=\int_{0}^{1}\int_{0}^{1}\mathbb{E}[B_t B_s]dtds \hspace{10mm} \text{(Fubini)}\\
    &= \int_{0}^{1}\int_{0}^{1}\min(s, t)dtds
\end{align*}
To examine the integral, notice that the integral runs from $[0,t]$ when $s<t$ and $[0,s]$ when $t<s$. Drawing this out shows that they occupy the same area. Thus, 
\begin{align*}
    \int_{0}^{1}\int_{0}^{1}\min(s, t)dtds &= \int_{0}^{1}\int_{0}^{t}sdsdt + \int_{0}^{1}\int_{0}^{s}tdtds \\
    &= 2\int_{0}^{1}\int_{0}^{t}sdsdt \\
    &= \frac{1}{3}
\end{align*}
We conclude $\int_{0}^{1}B_tdt\sim N(0, 1/3)$.

\vspace{2mm}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Find the distribution of $B_t + B_s$ where $s < t$.
\end{tcolorbox}
\textbf{Solution.} It is important to note that $B_t$ is not independent of $B_s$ so we can't simply sum them as if they were independent normal variables. Instead 
\begin{align*}
    B_t + B_s = B_s + (B_t - B_s) + B_s &= 2B_s + (B_t-B_s) \\
    &\stackrel{d}{=}N(0, 4s) + N(0, t-s) \\
    &\stackrel{d}{=}N(0, t + 3s)
\end{align*}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $B_t$ and $W_t$ be two independant Brownian Motions. Show that the process $X_t = \frac{B_t + W_t}{\sqrt{2}}$ also defines a Brownian Motion. Find the correlation between $X_t$ and $B_t$. 
\end{tcolorbox}
\textbf{Solution.} It is straightforward to verify that $X_t$ is a Brownian Motion. To find the correlation recall that 
$$\text{Corr}(X_t, B_t) = \frac{\text{Cov}(X_t, B_t)}{\sqrt{\text{Var($X_t$)}}\sqrt{\text{Var($B_t$)}}} = \frac{\text{Cov}(X_t, B_t)}{t}$$
To find the covariance of the two processes
\begin{align*}
    \text{Cov}(X_t ,B_t) &= \frac{1}{\sqrt{2}}(E[B_t^2] + E[B_tW_t]) \\
    &= \frac{t}{\sqrt{2}}
\end{align*}
From here, we conclude Corr$(X_t, B_t) = 1/\sqrt{2}$

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Prove that $B_t$ is nowhere differentiable a.s. [UniMelb]  
\end{tcolorbox}
\textbf{Proof.} We begin by noting 
\begin{align*}
    \frac{B_{t+\delta} - B_t}{\delta} \stackrel{d}{=} \frac{\sqrt{\delta} Z}{\delta} \stackrel{d}{=} \frac{Z}{\sqrt{\delta}}
\end{align*}
It is now clear that for any $K > 0$ fixed
\begin{align*}
    \mathbb{P}\left(\left|\frac{Z}{\sqrt{\delta}}\right| > K \right) =1 \quad \text{as } \quad \delta\to0. 
\end{align*}
Thus with probability 1, we conclude that $B_t$ is nowhere differentiable. $\square$

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} (Fractal Scaling Property of Brownian Motion) Show that for any fixed $a>0$  the process $X_t = aB_{t/a^2}$ is itself a Brownian Motion.  
\end{tcolorbox}
\textbf{Proof.} $X_t$ is clearly a Gaussian Process, as $B_{t/a^2}$ is a Gaussian Process. The Covariance function satisfies 
\begin{align*}
    \text{Cov}(X_t,X_s) = \mathbb{E}X_tX_s &= a^2\mathbb{E}B_{t/a^2}B_{s/a^2} \\
    &= a^2\min(t/a^2, s/a^2) \\
    &= \min(t,s)
\end{align*}
Thus, since $X_t$ is a Gaussian Process with covariance function $\min(t,s)$ we conclude that $X_t$ is a Brownian Motion. $\square$

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} State and Prove the Markov Property for Brownian Motion.
\end{tcolorbox}
\textbf{Solution.} The a process satisfies the Markov Property (is Markovian) if the future trajectory of the process is independent of its past. That is, the future evolution of the process depends only on its current position, and no further history. Mathematically, 
$$P(X_{t+s} \leq y| \mathcal{F}_t) = P(X_{t+s} \leq y| X_t=x)$$
To see that Brownian Motion satisfies the Markov Property let $s <t$ and consider 
\begin{align*}
    P(B_t \leq y| \mathcal{F}_s) &= P(B_s + (B_t-B_s) \leq y|\mathcal{F}_s) \\
    &= P(B_t \leq y| B_s =x) \\
\end{align*}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $s < t$, find the distribution of $B_s | B_t = b$.
\end{tcolorbox}
\textbf{Solution.} The \textit{trick} to this problem is to recognize that $(B_s, B_t)$ is multivariate normal then apply the \href{https://online.stat.psu.edu/stat505/lesson/6/6.1}{formulae for conditional distributions of multivariate Gaussians}. That is: 
\begin{align*}
    & E[B_s | B_t=b] = \frac{s}{t}b \\
    &\text{Var}(B_s | B_t=b) = s-\frac{s^2}{t} = \frac{s(t-s)}{t}
\end{align*}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]

\textbf{Question:} Let $u<s < t$, find the distribution of $B_s | B_u=a,B_t = b$.
\end{tcolorbox}
\textbf{Solution.} 


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} By considering $M_t := \max_{0\leq s \leq t} B_s$ and $T_a$ be the first time that $B_t$ hits $a > 0$. Prove that $P(\tau_x\leq t) = P(|B_t|\geq x)$  
\end{tcolorbox}
\textbf{Proof.} Begin by noting that $\{T_a\leq t\} = \{M_t\geq a\}$. Then
\begin{align*}
    \mathbb{P}(T_a\leq t) &= \mathbb{P}(T_a\leq t, B_t\geq a) + \mathbb{P}(T_a\leq t, B_t< a) \\
    &= 2\mathbb{P}(T_a\leq t, B_t\geq a) \hspace{10mm} \text{Reflection Principle} \\
    &= 2\mathbb{P}(M_t\geq a, B_t\geq a) \\
    &= 2\mathbb{P}(B_t\geq a) \\
    &= \mathbb{P}(B_t> a) + \mathbb{P}(B_t <-a) \\
    &= \mathbb{P}(|B_t| > a)
\end{align*}
Therefore, $M_t \stackrel{d}{=} |B_t|$ as claimed. $\square$

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Prove the law of large numbers for Brownian Motion
$$\lim_{t\to\infty}\frac{B_t}{t} = 0 \text{ a.s.}$$
\end{tcolorbox}
\textbf{Proof.}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Prove the law of the iterated logarithm for Brownian Motion:
\begin{align*}
    & \limsup_{t\to\infty}\frac{B_t}{\sqrt{t\ln\ln t}}=1\quad \text{a.s} \\
    & \liminf_{t\to\infty}\frac{B_t}{\sqrt{t\ln\ln t}}=-1\quad \text{a.s}
\end{align*}
\end{tcolorbox}
\textbf{Proof.}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Argue that the process $X_t := \int_0^t B_sds$ exists. Show that it is a Gaussian Process, state its mean, variance and Covariance Function.
\end{tcolorbox}
\textbf{Proof.} First note that $\int_0^t B_sds$ is not an Itô integral, but a regular Riemann Integral. Since the map $t\mapsto B_t$ is continuous, it is Riemann Integrable, making $X_t$ well defined. To see that $X_t$ is a Gaussian Process, we have from the definition of Riemann Integration
$$\int_0^t B_sds = \lim_{n\to\infty}\sum_{i=1}^{n}B_{t_i}\Delta t_i$$
Since $B_{t_i}$ is a Gaussian Process and Gaussian Process's are conserved under the limit, we conclude that $X_t$ is a Gaussian Process too. For the mean, by linearity of expectation we have 
$$\mathbb{E}X_t = \int_0^t\mathbb{E}B_sds = 0$$
Therefore, the variance is now just the second moment so 
$$\mathbb{E}X_t^2 = \int_0^t\int_0^t\mathbb{E}B_sB_rdsdr = \int_0^t\int_0^t\min(s,r)dsdr$$
The integral is symmetric over the region $[0,t]$ so we can take $s<r$ and evaluate the integral as
\begin{align*}
    \int_0^t\int_0^t\min(s,r)dsdr = 2\int_0^t\int_0^r sdsdr = \frac{t^3}{3}
\end{align*}
Finally, we evaluate the covariance function$


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Show that for any $x\geq 0$ 
$$\mathbb{P}(\max_{s\in[0,t]} B_t\geq x) = 2\left(1- \Phi\left(\frac{x}{\sqrt{t}}\right)\right).$$
\end{tcolorbox}
\textbf{Proof.} Define $\tau_x := \inf_{t\geq0}\{B_t=x\}$ and follow the usual steps for deriving the maximum of Brownian Motion: 
\begin{align*}
    \mathbb{P}(\tau_x \leq t) &= \mathbb{P}(\tau_\alpha\leq t, B_t \geq x) + \mathbb{P}(\tau_x\leq t, B_t < x) \\
    &= 2\mathbb{P}(\tau_x\leq t, B_t \geq x) \\
    &= 2\mathbb{P}(X_t\geq x, B_t\geq x) \hspace{10mm}\text{{Substituting $\{X_t\geq\alpha\} = \{\tau_\alpha \leq t\}$}} \\
    &= 2\mathbb{P}(B_t\geq x) \\
\end{align*}
We're now pretty much done. All that remains is to write this in the desired form
\begin{align*}
    2\mathbb{P}(B_t\geq x) = 2(1 - \mathbb{P}(B_t <x))  &= 2(1 - \mathbb{P}(B_t/\sqrt{t} <x/\sqrt{t})) \\
    &= 2\left(1- \Phi\left(\frac{x}{\sqrt{t}}\right)\right) \quad\square
\end{align*}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Show that for any $x<0$ 
$$\mathbb{P}(\min_{s\in[0,t]} B_t\leq x) = 2\mathbb{P}(B_t\leq x).$$
\end{tcolorbox}
\textbf{Proof.} For $x > 0$ we have by the reflection principle 
$$\mathbb{P}(\max_{s\leq t}B_s\geq x) = 2\mathbb{P}(B_t\geq x)\quad\quad (1)$$
Recall that for any standard Brownian Motion, $-B_t$ is also a standard Brownian motion, so $\max_{s\leq t}B_s$ has the same distribution as $\max_{s\leq t}-B_s$. It then follows from $(1)$ for $x>0$
$$\mathbb{P}(\max_{s\leq t}-B_s) = 2\mathbb{P}(-B_t\geq x)$$
Finally, recall that $-\max_{s\leq t} -B_s = \min_{s\leq t} B_s$ so we have 
$$\mathbb{P}(\min_{s\leq t}B_s\leq-x) = 2\mathbb{P}(B_t\leq -x)$$
Since $x>0$ was arbitrary, we can write the above as $\mathbb{P}(\min_{s\leq t}B_s\leq y) = 2\mathbb{P}(B_t\leq y)$ for arbitrary $y<0$. $\square$


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $\tau_w :=\inf\{t\geq 0 : W_t=w\}$ for a standard Brownian Motion $W_t$. Using the reflection principle, show that 
$$\mathbb{P}(\tau_w\leq t) = 1 + \Phi\left(-\frac{|w|}{\sqrt{t}}\right) - \Phi\left(\frac{|w|}{\sqrt{t}}\right)$$
\end{tcolorbox}
\textbf{Proof.} This exercise requires a bit of care, and needs to be split up into the case where $w\geq0$ and $w<0$. Starting with the first case, by the reflection principle for $w\geq0$
\begin{align*}
    \mathbb{P}(T_w\leq t) &= 2\mathbb{P}(B_t\geq w)  \\
    &= \mathbb{P}(B_t > w) + \mathbb{P}(B_t < -w) \\
    &= 1 - \Phi\left(\frac{w}{\sqrt{t}}\right) + \Phi\left(-\frac{w}{\sqrt{t}}\right)
\end{align*}
Next, we assume that $w < 0$. Since $-B_t$ is also a standard Brownian Motion, we have similar to before 
\begin{align*}
    \mathbb{P}(T_w\leq t)&= 2\mathbb{P}(-B_t<w) \hspace{10mm} \text{for $w<0$} \\
    &= \mathbb{P}(-B_t<w) + \mathbb{P}(B_t > -w) \\
    &= \mathbb{P}(-B_t<w) + 1-\mathbb{P}(B_t\leq-w) \\
    &= 1-\Phi\left(-\frac{w}{\sqrt{t}}\right)+\Phi\left(\frac{w}{\sqrt{t}}\right)
\end{align*}
Combine both cases to have the result. $\square$



\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Find the joint density of $M_t := \max_{s\leq t} B_s$ and $B_t$. \cite{Fima}
\end{tcolorbox}
\textbf{Solution.} Let $y > x$ and define $\tau_y := \inf_{t}\{B_t \geq y\}$. As usual, note that $\{M_t \geq y\} = \{\tau_y\leq t\}$. Consider 
$$P(M_t\geq y, B_t\leq x) = P(\tau_y\leq t, B_t\leq x)$$
This means that, $B_t$ hits some level $y$ then descends back below $x$ on $\{\tau_y\leq t\}$. Recall that by the reflection principle, we now have 
\begin{align*}
    \{\hat{B}_t \leq x\} &= \{2B_{\tau} - B_t \leq x\} \\
    &= \{B_t\geq 2B_\tau - x\} \\
    &= \{B_t\geq 2y - x\}
\end{align*}
so, 
\begin{align*}
    P(\tau_y\leq t, B_t\leq x) &= P(\tau_y\leq t, B_t\geq2y- x) \\
    &= P(B_{t}\geq 2y-x) \\
    &= 1 - \Phi\left(\frac{2y-x}{\sqrt{t}}\right)
\end{align*}
As we are required to obtain the density, we now applying differentiation
\begin{align*}
    f_{M_t, B_t}(y,x) = \frac{\partial^2}{\partial y\partial x}\left[-\Phi\left(\frac{2y-x}{\sqrt{t}}\right)\right] &= \frac{\partial}{\partial y}\left[\frac{1}{\sqrt{t}}\phi\left(\frac{2y-x}{\sqrt{t}}\right)\right] \\
    &= \sqrt{\frac{2}{\pi}}\frac{(2y-x)}{t^{3/2}}\exp\left(-\frac{1}{2t}(2y-x)^2\right)
\end{align*}
Which is defined for $y\geq 0$ and $x\leq y$

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $B_t$ be a standard Brownian Motion. Show that $\text{Cov}(B_t,B_s) = \min\{s,t\}$ and deduce the correlation coefficient of $B_s$ and $B_t$ is $$\rho=\sqrt{\frac{\min\{s,t\}}{\max\{s,t\}}}$$
\end{tcolorbox}
\textbf{Proof.} For the covariance, assume $s<t$ then
\begin{align*}
    \text{Cov}(B_t,B_s) = \mathbb{E}[B_sB_t] &= \mathbb{E}[B_s^2+B_s(B_t-B_s)] \\
    &= E[B_s^2] \\
    &=s
\end{align*}
since $s<t$ was arbitrary, we conclude Cov$(B_s,B_t)$ = $\min(s,t)$. For the coefficient of correlation
\begin{align*}
    \rho = \frac{\text{Cov}(B_t,B_s)}{\sqrt{\text{Var}(B_t)\text{Var}(B_s)}} = \frac{\min(s,t)}{\sqrt{st}}
\end{align*}
Suppose $s<t$ then 
$$\rho = \frac{s}{\sqrt{st}} = \sqrt{\frac{s}{t}}$$
and analogously for if $t<s$. We therefore conclude that  $\rho=\sqrt{\frac{\min\{s,t\}}{\max\{s,t\}}}$. $\square$

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} (Time Reversal) let $W_t: t\geq0$ be a standard Brownian Motion. Show that under the time reversal $B_t := W_1 - W_{1-t}$is also a standard Brownian Motion.
\end{tcolorbox}
\textbf{Proof.} $B_t$ is the sum of contiuos processes' so is itself continuous. 

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $\{W_t : t\geq0 \}$ be a standard Brownian Motion. Show that the covariance of $\int_0^s W_udu$ and $\int_0^t W_vdv$ satisfies
$$\text{Cov}\left(\int_0^s W_udu, \int_0^t W_vdv\right) = \frac{1}{3}\min\{s^3, t^3\} + \frac{1}{2}|t-s|\min\{t^2, s^2\}$$
\end{tcolorbox}
\textbf{Proof.}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $\{W_t : t\geq0 \}$ be a standard Brownian Motion. Show that it has finite quadratic variation such that 
$$[W,W]_t = \lim_{n\to\infty}\sum_{i=0}^{n-1}(W_{t_{i+1}} - W_{t_i})^2 = t$$
Then, deduce that $dW_t\cdot dW_t = dt.$
\end{tcolorbox}
\textbf{Proof.} We can use an interesting strategy to show this: We'll set the random variable $X_t := [W,W]_t$ then show that $\mathbb{E}X_t = t$ and $\text{Var}(X_t)=0$ as $n\to\infty$ leaving us with $X_t=t$ in probability.



\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Show that $|B_t| \stackrel{d}{=} \max_{0\leq s\leq t} B_s$.
\end{tcolorbox}
\textbf{Proof.} For notional simplicity let $X_t := \max_{s\in[0,t]} B_s$ and define the stopping time for the first time $B_t$ hits the level $\alpha$. That is, $\tau_\alpha := \inf_{t>0}\{B_t = \alpha\}$. One can see that $\{X_t\geq\alpha\} = \{\tau_\alpha \leq t\}$. We can now calculate
\begin{align*}
    \mathbb{P}(\tau_\alpha \leq t) &= \mathbb{P}(\tau_\alpha\leq t, B_t \geq \alpha) + \mathbb{P}(\tau_\alpha\leq t, B_t < \alpha) \\
    &= 2\mathbb{P}(\tau_\alpha\leq t, B_t \geq \alpha) \\
    &= 2\mathbb{P}(X_t\geq \alpha, B_t\geq \alpha) \hspace{10mm}\text{{Substituting $\{X_t\geq\alpha\} = \{\tau_\alpha \leq t\}$}} \\
    &= 2\mathbb{P}(B_t\geq \alpha) \\
    &= \mathbb{P}(B_t\geq \alpha) + \mathbb{P}(B_t\leq -\alpha) \\
    &=\mathbb{P}(|B_t|\geq\alpha)
\end{align*}
Thus, we can see that $\mathbb{P}(X_t\geq\alpha) = \mathbb{P}(|B_t|\geq\alpha)$ to complete the Proof. $\square$

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Calculate $\mathbb{E}[B_sB_uB_t]$ for $0<s<t<u.$
\end{tcolorbox}
\textbf{Solution.} The vector $(B_s, B_t, B_u)\sim N(0,\Sigma)$ where 
\[
\Sigma = \begin{pmatrix}
E[B_s^2] & E[B_s B_t] & E[B_s B_u] \\
E[B_t B_s] & E[B_t^2] & E[B_t B_u] \\
E[B_u B_s] & E[B_u B_t] & E[B_u^2]
\end{pmatrix}
= 
\begin{pmatrix}
s & s & s \\
s & t & t \\
s & t & u
\end{pmatrix}.
\]
One can then invoke \href{https://en.wikipedia.org/wiki/Isserlis%27s_theorem}{Isserlis' Theorem} to deduce that, since the length of the product is odd, the mean must be 0. \\
\\
The alternative approach is to tackle it algebraically. Since $0<s<t<u$ we write
\begin{align*}
    B_sB_tB_u &= B_s(B_s+(B_t-B_s))(B_t+(B_u-B_t)) \\
    &= (B_s^2 + B_s(B_t-B_s))(B_t+(B_u-B_t)) \\
    &= B_s^2B_t + B_s^2(B_u-B_t)+B_sB_t(B_t-B_s) + B_s(B_t-B_s)(B_u-B_t) \quad\quad (1)
\end{align*}
Since $B_t\perp B_u-B_t$ and $B_s \perp B_t-B_s \perp B_u-B_t$ when we take expectation of (1) we are left with
$$\mathbb{E}[B_s^2B_t] + \mathbb{E}[B_s B_t(B_t-B_s)]$$
Splitting up the first term
$$B_s^2B_t = B_s^2 (B_s + (B_t-B_s)) = B_s^3 + B_s^2(B_t-B_s)$$
Which after taking expectation, we are left with $\mathbb{E}[B_s^3].$ For the second term, we have 
\begin{align*}
    B_sB_t(B_t-B_s)&=B_s(B_s+(B_t-B_s))(B_t-B_s) \\
    &=B_s^2(B_t-B_s)+B_s(B_t-B_s)^2
\end{align*}
which vanishes after taking expectation. Thus we are left with 
$$\mathbb{E}[B_sB_tB_u] = \mathbb{E}[B_s^3]=0$$
Since it is an odd moment of a $N(0,s)$ Gaussian distribution.


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
    \textbf{Question:} Let $T_x :=\inf\{s\leq t: B_s=x\}$. Show that the probability density of $T_x$ is given by
    $$f_{T_x}(t) = \frac{|x|}{\sqrt{2\pi}} t^{-3/2} e^{\frac{-x^2}{2t}}$$
\end{tcolorbox}
\textbf{Proof.}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
    \textbf{Question:} Show that 
    $$P(B_s>0,B_t>0) = \frac{1}{4} + \frac{1}{2\pi}\arcsin{\sqrt{\frac{s}{t}}} \quad s<t$$
\end{tcolorbox}
\textbf{Proof.}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
    \textbf{Question:} Derive the distribution of $B_t$  given the maximum $\max_{s\leq t} B_s$
\end{tcolorbox}
\textbf{Solution.}


\newpage
\section{Brownian Motion Calculus}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let the Itô integral of $W_tdW_t$ be defined by the following limit
$$I(t) = \int_0^tW_sdW_s = \lim_{n\to\infty}\sum_{i=0}^{n-1}W_{t_i}(W_{t_{i+1}} - W_{t_i})$$
Show that $I(t) = \frac{1}{2}(W_t^2-t)$.
\end{tcolorbox}
\textbf{Proof.} Completing the square on $W_{t_i}(W_{t_{i+1}} - W_{t_i})$
\begin{align*}
    I_t &= \lim_{n\to\infty}\sum_{i=0}^{n-1}W_{t_i}(W_{t_{i+1}} - W_{t_i}) \\
    &= \lim_{n\to\infty}\sum_{i=0}^{n-1}\frac{1}{2}((W_{t_{i+1}}^2-W_{t_i}^2) - (W_{t_{i+1}} - W_{t_i})^2) \\
    &= \frac{1}{2}\left(\lim_{n\to\infty}\sum_{i=0}^{n-1}W_{t_{i+1}}^2-W_{t_i}^2 - \lim_{n\to\infty}\sum_{i=0}^{n-1}(W_{t_{i+1}}-W_{t_i})^2\right) \\
    &= \frac{1}{2}\left(\lim_{n\to\infty}(W_{t_n}^2 - W_{t_0}^2) -  \lim_{n\to\infty}\sum_{i=0}^{n-1}(W_{t_{i+1}}-W_{t_i})^2\right)
\end{align*}
for the first term $W_{t_n}^2\to W_t^2$ as $n\to\infty$. For the second term, one recognizes this is the definition of the Quadratic Variation for Brownian Motion, so the entire expression collapses to $I_t = \frac{1}{2}(W_t^2-t)$. $\square$

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Recall that the stochastic Itô integral with respect to standard Brownian Motion for a simple function $f$ is defined as 
$$I_t = \int_0^t f(W_s,s)dW_s = \lim_{n\to\infty}\sum_{i=1}^{n-1}f(W_{t_i}, t_i)(W_{t_{i+1}} - W_{t_i})$$
Using the properties of standard Brownian Motion, show that $I_t$ is a martingale.
\end{tcolorbox}
\textbf{Proof.}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Recall that the stochastic Itô integral with respect to standard Brownian Motion for a simple function $f$ is defined as 
$$I_t = \int_0^t f(W_s,s)dW_s = \lim_{n\to\infty}\sum_{i=1}^{n-1}f(W_{t_i}, t_i)(W_{t_{i+1}} - W_{t_i})$$
Using the properties of the standard Brownian Motion, show that the Itô Integral process has mean 0. That is, 
$$\mathbb{E}\left(\int_0^t f(W_s,s)dW_s\right) =0$$
\end{tcolorbox}
\textbf{Proof.}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $X_t=-1$ on $0\leq t\leq1$, $X_t=1$ on $1 < t\leq2$ and $X_t=2$ on $2 < t\leq3$. Find the distribution of $\int_0^3X_sdB_s$. \cite{Fima}
\end{tcolorbox}
\textbf{Solution.} This is an Itô integral for a simple, nonstochastic process. By Definition
\begin{align*}
    \int_0^3 X_sdB_s &:= -(B_1-B_0) + (B_2-B_1)+2(B_3-B_2) \\ 
    &= 2B_3 -B_2-2B_1 \\
\end{align*}
We now the distribution will be gaussian with 0 mean. To find the variance we recall Cov$(B_t, B_s) = \min(t,s)$ and so
\begin{align*}
    \text{Var}(2B_3 -B_2-2B_1) &= \text{Var}(2B_3) + \text{Var}(B_2) + \text{Var}(2B_1) - 2\text{Cov}(B_3, B_2)-4\text{Cov}(B_3, B_2) +2\text{Cov}(B_2,B_1) \\
    &= 12 + 2 + 4-4-8+2 \\
    &= 6
\end{align*}
Hence we conclude $\int_0^3X_sdB_s\sim N(0,6).$


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} By finding a suitable sequence of Simple Adapted Processes, find $\int_0^T B_s dB_s$. \cite{Fima}
\end{tcolorbox}
\textbf{Solution.} Let $0=t_0<t_1<\dots,t_n=t$ and write
\begin{align*}
    \int_0^tB_sdB_s &= \lim_{n\to\infty}\sum_{i=0}^{n-1}B_{t_i}(B_{t_{i+1}} - B_{t_i}) \\
    &= \lim_{n\to\infty}\sum_{i=0}^{n-1}\frac{1}{2}(B_{t_{i+1}}^2 - B_{t_i}^2 - (B_{t_{i+1}} - B_{t_i})^2) \quad\quad \text{by completing the square} \\
    &= \frac{1}{2}\left(\lim_{n\to\infty}B_{t_{n}}^2 - B_{t_0}^2 - \lim_{n\to\infty}\sum_{i=0}^{n-1}(B_{t_{i+1}} - B_{t_i})^2\right) \\
    &= \frac{1}{2}(B_t^2-t)
\end{align*}
Where the final inequality comes from the definition of quadratic variation for Brownian Motion. 


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Find the values of $\alpha$ for which the process $Y_t = \int_0^t(t-s)^{-\alpha}dB_s$ is defined.
\end{tcolorbox}
\textbf{Solution}. Recall that $\int_0^t X_sdB_s$ is well-defined so long as $\int_0^t E[X_s^2]ds<\infty$. Applying this to $Y_t$ we require $\int_0^t(t-s)^{-2\alpha}ds <\infty$. Make the substitution $u = (t-s)$ then\footnote{Notice that $s$ ranges from $[0,t]$ in the original integrand, so when we make the $u$ substitution under the original end-points $u = t-s$ now varies from $t$ down to $0$. We then switch the order of these bounds which multiples through by $-1$.}
\begin{align*}
    \int_0^t(t-s)^{-2\alpha}ds &= \int_t^0 u^{-2\alpha}(-du) \\
    &= \int_0^tu^{-2\alpha}du
\end{align*}
From here, one clearly sees this integral is finite provided $\alpha < 1/2.$ $\square$

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Find the Stochastic Differential $d(Y_t / Z_t)$ where $Y_t = tB_t$ and $Z_t=e^{B_t}$ using Itô's formula. \cite{Fima}
\end{tcolorbox}
\textbf{Solution.} let $f(y,z) = y/z$. 


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Derive the Stochastic Calculus equivalent of the Product Rule by applying Itô's lemma. 
\end{tcolorbox}
\textbf{Solution.} Consider the function $f(x,y) = xy$. One verifies it has the partial derivatives $f_x(x,y)=y$, $f_y(x,y)=x$, $f_{x,y}(x,y)=1$ and vanishing second-order derivatives. Applying Ito's formula for multi-variate functions one finds
$$dX_tY_t = Y_tdX_t + X_tdY_t + \sigma_X(t)\sigma_Y(t)dt.$$

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Suppose $X_t = (1-t)\int_0^t\frac{dB_s}{1-s}$, where $0\leq t<1$. Find $dX_t$
\end{tcolorbox}
\textbf{Solution.} 


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Find the quadratic variation of $X_t = tB_t$
\end{tcolorbox}
\textbf{Solution.} When asked to find the quadratic varition of a process, we should first ask if the process if an Itô process? If so, it can be written as $dX_t = \mu_t dt + \sigma_t dB_t$ and we immediately know 
$$[X, X]_t = \int_0^t\sigma_s^2ds.$$
For our process in question, we determine if it is an Itô process by applying Itô's lemma to the function $f(x,y) = xy$ with arguments $x=t, y=B_t$. 
\begin{align*}
    dX_t = df(t,B_t) &= \frac{\partial f}{\partial B_t}dB_t + \frac{\partial f}{\partial t}dt + \frac{\partial^2 f}{\partial B_t^2}dt \\
    &=tdB_t + B_tdt
\end{align*}
Thus, $X_t$ is an Itô process with $\sigma_t = t$. It follows that the quadratic variation is therefore
\begin{align*}
    [X,X]_t = \int_0^t s^2ds = \frac{t^3}{3} \quad \square
\end{align*}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $W_t$ be a standard Brownian Motion. Suppose $X_t$ is some process that satisfies $d(\frac{1}{X_t}) = \frac{1}{X_t}(2dt-dW_t)$. It is known that $dX_t = X_t(adt + bdW_t)$. Find $a$ and $b$.
\end{tcolorbox}
\textbf{Solution.} Applying Ito's Chain Rule to $f(t) = 1/t$ we have 
\begin{align*}
    d\left(\frac{1}{X_t}\right) = df(X_t) &=  f^\prime(X_t)dX_t + \frac{1}{2}f^{\prime\prime}(X_t)b^2X_t^2dt \\
    &= -\frac{1}{X_t^2}(aX_tdt + bX_tdW_t) + \frac{b^2}{X_t}dt \\
&= \frac{1}{X_t}(b^2-a)dt-bdW_t
\end{align*}
From here, it is clear that $2 = b^2-a$ and $-1 = b$. Thus, $a=b=-1$. $\square$

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $X_t = \int_0^t (ts)^2dB_s$. Find $dX_t$ and the quadratic variation $[X,X](t)$.
\end{tcolorbox}
\textbf{Solution.} Let $f(t,s) = t^2s^2$. By the Leibniz rule 
\begin{align*}
    dX_t &= f(t,t)dB_t + \int_0^t \frac{\partial}{\partial t}f(s,t) dB_sdt  \\
         &= t^4dB_t + \int_0^t 2ts^2dB_sdt \\
         &= t^4dB_t + 2t \left(\int_0^t s^2dB_s\right)dt
\end{align*}
For the Quadratic Variation, $X_t$ is an Itô process with $\sigma(s,t) = t^2 s^2$ so
$$[X,X]_t = \int_0^t \sigma(s,t)^2 ds = t^4\int_0^t s^4ds = \frac{t^9}{5}$$

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $W_t : t\geq0$ be a Wiener process the stochastic integral with respect to a standard Wiener process can be defined as 
$$\int_0^ts dW_s = \lim_{n\to\infty}\sum_{i=0}^{n-1}t_i(W_{t_{i+1} } - W_{t_i})$$
Show that 
$$\int_0^tsdW_s = tW_t - \int_0^tW_sds.$$
\end{tcolorbox}
\textbf{Proof.} One can apply Itô's lemma to $tW_t$ for a simple solution. Using the definition provided


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Consider the process 
$$
B_t = \begin{cases}
    0 & t=0 \\
    \frac{\sqrt{3}}{t}\int_0^tW_s ds & t>0
\end{cases}
$$
is $B_t$ a standard Wiener process? 
\end{tcolorbox}
\textbf{Solution.}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Using integration by parts, show that 
$$\int_0^tW_sds = \int_0^t(t-s)dW_s$$
and prove that $\int_0^t W_sds\sim N(0, t^3/3)$. 
\end{tcolorbox}
\textbf{Proof.} Consider $X_t = tW_t$. Stochastic integration by parts comes from the product rule: 
$$dX_t = dtW_t = tdW_t +W_tdt$$
Integrating both sides 
$$X_t - X_0 = \int_0^t sdW_s + \int_0^t W_sds$$
Since $X_0 = 0$ we can move the first term on the right hand side over and substitute $X_t = tW_t$ to find 
\begin{align*}
    tW_t - \int_0^tsdW_s = \int_0^t(t-s)dW_s = \int_0^tW_sds.
\end{align*}
The Itô integral $\int_0^t(t-s)dW_s$ is Gaussian, so we know that $\int_0^tW_sds$ must be normally distributed. From the properties of the Itô integral, we know that the mean is zero and variance is given by 
\begin{align*}
    \text{Var}\left(\int_0^tW_sds\right) &= \text{Var}\left(\int_0^t(t-s)dW_s\right) \\
    &= \int_0^t(t-s)^2ds \\
    &= \frac{t^3}{3}
\end{align*}
So $\int_0^tW_sds\sim N(0,t^3/3)$. $\square$

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $X_t = \int_0^t(t-s)dB_s$. Find $dX_t$ and the quadratic variation $[X,X]_t$. Compare to the Quadratic Variation of Itô Integrals.
\end{tcolorbox}
\textbf{Proof.}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Show that 
$$\mathbb{E}\left[\int_0^TX_tdB_t\int_0^TY_tdB_t\right] = \int_0^T\mathbb{E}Y_tX_tdt.$$
\end{tcolorbox}
\textbf{Proof.}


\newpage
\section{Stochastic Differential Equations}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $\Phi$ be the standard normal cumulative distribution functoin. Show that the process $$X_t := \Phi\left(\frac{B_t}{\sqrt{T-t}}\right)$$ for $0\leq t<T$ is a martingale for a fixed $T$.
\end{tcolorbox}
\textbf{Proof.} We apply Itô's lemma to the function $f(x,t) = \Phi(\frac{x}{\sqrt{T-t}})$. The partial derivatives are 
\begin{align*}
    & \frac{\partial f}{\partial x} = \frac{1}{\sqrt{T-t}} \phi\left(\frac{x}{\sqrt{T-t}}\right) \\
    & \frac{\partial^2 f}{\partial x^2} = - \frac{x}{(T-t)^{\frac{3}{2}}} \phi\left(\frac{x}{\sqrt{T-t}}\right) \quad\quad \text{(using $\phi^\prime(x) = -x\phi(x)$)} \\
    & \frac{\partial f}{\partial t} = -\frac{x}{(T-t)^{\frac{3}{2}}}\phi\left(\frac{x}{\sqrt{T-t}}\right)
\end{align*}
From the above, we see the term $dt$ term in $df(B_t,t)$ will vanish, so we are left with 
$$df(B_t,t) = \frac{1}{\sqrt{T-t}} \phi\left(\frac{B_t}{\sqrt{T-t}}\right)dB_t$$
Thus, $X_t$ is a martingale. $\square$




\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Show that $X_t = X_0 e^{(\mu-\sigma^2/2)t + \sigma B_t}$ satisfies $dX_t =\mu X_t dt + \sigma X_t dB_t$.
\end{tcolorbox}
\textbf{Proof.} The classic way to show this is to start with the stochastic differential and apply Itô's lemma with $f(t) = \ln t$
\begin{align*}
    d\ln X_t  &= \frac{1}{X_t}(\mu X_t dt + \sigma X_t dB_t) - \frac{\sigma^2}{2}dt \\
              &= \left(\mu - \frac{\sigma^2}{2}\right)dt + \sigma dB_t
\end{align*}
Now by integrating both sides
\begin{align*}
    \ln X_t &= \ln X_0 + \left(\mu - \frac{\sigma^2}{2}\right)t + \sigma B_t \\
    &= X_0 \exp^{\left(\mu - \frac{\sigma^2}{2}\right)t + \sigma B_t}.
\end{align*}
Which completes the Proof. $\square$

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Show that the solution to the \textit{Ornstein-Uhlenbeck} Process $dX_t = -\alpha X_tdt + \sigma dB_t$ is given by 
$$X_t = e^{-\alpha t}\left(X_0 + \int_0^t\sigma e^{\alpha s}dB_s\right)$$
\end{tcolorbox}
\textbf{Proof.} Apply Itô's lemma to $Y_t := e^{\alpha t} X_t$ 
\begin{align*}
    df(B_t, t) &= e^{\alpha t}(-\alpha X_tdt + \sigma dB_t) + \alpha e^{\alpha t}X_t dt \\
               &= \sigma e^{\alpha t} dB_t
\end{align*}
set $dX_t = df(B_t,t)$ and integrate both sides to find 
$$X_t = e^{-\alpha t}\left(X_0 + \int_0^t e^{\alpha s}dB_s\right)$$

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Show that the unique solution to $dU_t = U_tdX_t$ is given by 
$$\mathcal{E}(X_t) := U_t = e^{X_t - X_0 - \frac{1}{2}[X,X]_t}.$$
\end{tcolorbox}
\textbf{Proof.}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Solve the SDE $dX_t = X_tB_tdt + X_tB_tdB_t, \quad X_0=1$.
\end{tcolorbox}
\textbf{Solution.} Start by writing
\begin{align*}
    dX_t &= X_tB_tdt + X_tB_tdB_t \\
    &= X_tdY_t
\end{align*}
Where $dY_t := B_tdt + B_tdB_t$. This is now in the form of a stochastic exponential which we know has unique solution
$$X_t = X_0\exp\left(Y_t - Y_0 + \frac{1}{2}[Y,Y]_t\right).$$
Since $Y_t$ is an Itô Process with $\sigma_s = B_s$
$$[Y,Y]_t = \int_0^t B_s^2 ds$$
Thus, the solution to the SDE is 
\begin{align*}
    X_t &= \exp\left( \int_0^tB_sds + \int_0^t B_sdB_s - \frac{1}{2}\int_0^t B_s^2ds\right). \\
    &= \exp\left( \int_0^tB_s - \frac{B_s^2}{2}ds + \int_0^t B_sdB_s \right). \quad \square
\end{align*}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Solve the SDE $dX_t = X_t dt + B_tdB_t, \quad X_0=1$. Comment on whether it is a diffusion type SDE. 
\end{tcolorbox}
\textbf{Solution.} It's not a diffusion type SDE as the $dB_t$ term doesn't contain only on $t$ and $X_t$. To solve the SDE, apply Itô's lemma to $f(x,t) = e^{-t}x$ then 
$$de^{-t}X_t = e^{-t}B_tdB_t$$
Integrating both sides and rearranging we find 
$$X_t = e^t\left(X_0 + \int_0^te^{-s}B_sdB_s\right)$$


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Find $d\mathcal{E}(B_t)^2$ where $\mathcal{E}$ denotes the Stochastic Exponential.
\end{tcolorbox}
\textbf{Solution.} Recall that $\mathcal{E}(X_t) = \exp(X_t - X_0 + \frac{1}{2}[X,X]_t)$. So $\mathcal{E}(B_t) = \exp(B_t - t/2).$ We 

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} For a constant $\theta$ find the SDE for the process $X_t = e^{\theta W_t - \frac{1}{2}\theta^2 t}$. By writing the SDE in integral form calculate $\mathbb{E}[e^{\theta W_t}]$, the moment generating function of the standard Wiener process. 
\end{tcolorbox}
\textbf{Solution.}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Suppose $X_t$ follows the generalised SDE 
$$dX_t = \mu(X_t,t)dt + \sigma(X_t,t)dW_t$$
Show that $X_t$ is a martingale if $\mu(X_t,t)=0.$
\end{tcolorbox}
\textbf{Proof.}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} (Bachelier Model / Arithmetic Brownian Motion). Suppose that $X_t$ follows Arithmetic Brownian Motion with the SDE
$$dX_t = \mu dt + \sigma dW_t$$
Taking integrals show that for $t<T$
$$X_T = X_t + \mu(T-t)+\sigma W_{T-t}$$
Then deduce that $X_T|X_t=x\sim N(x+\mu(T-t),\sigma^2(T-t)).$
\end{tcolorbox}
\textbf{Proof.} Integrating both sides 
$$\int_t^TdX_T = \int_t^T\mu dt + \int_t^T\sigma dW_t \implies X_T-X_t=\mu(T-t)+\sigma W_{T-t}$$
where $W_{T-t} = W_T- W_t$. From here, it is trivial to condition on $X_t=x$ to find the distribution. $\square$


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} (Generalised Brownian Motion). Suppose that $X_t$ follows Generalised Geometric Brownian Motion with the SDE
$$dX_t = \mu_tX_tdt + \sigma_tX_t dW_t$$
Through Itô's lemma and taking integrals show that for $t<T$
$$X_T = X_t\exp\left(\int_t^T\left(\mu_s-\frac{1}{2}\sigma_s^2\right)ds + \int_t^T\sigma_s dW_s\right)$$
Find the mean and variance of $X_T$.
\end{tcolorbox}
\textbf{Proof.}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} (Generalised Ornstein-Uhlenbeck Process). Suppose that $X_t$ follows the Ornstein-Uhlenbeck process with SDE
$$dX_t = \kappa(\theta-X_t)dt + \sigma dW_t$$
By applying Itô's formula to $Y_t=e^{\kappa t}X_t$ and taking integrals show that for $t<T$,
$$X_T = X_te^{-\kappa(T-t)}+\theta[1-e^{-\kappa(T-t)}] + \int_t^T\sigma e^{-\kappa(T-s)}dW_s$$
Find the mean and variance of $X_T | X_t=x$ and deduce that $X_T$ follows a normal distribution.
\end{tcolorbox}
\textbf{Proof.}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} (Cox-Ingersoll-Ross (CIR) Model). Suppose that $X_t$ follows the so-called CIR model with SDE
$$dX_t =\kappa(\theta-X_t)dt + \sigma\sqrt{X_t}dW_t$$
With $X_0>$. By applying Itô's formula to $Z_t=e^{\kappa t}X_t$ and taking integrals show that for $t<T$,
$$X_T = X_te^{-\kappa(T-t)}+\theta[1-e^{-\kappa(T-t)}] + \int_t^T\sigma e^{-\kappa(T-s)}\sqrt{X_s}dW_s$$
Find the second moment of $X_T$, and the mean and variance of $X_T | X_t=x$.
\end{tcolorbox}
\textbf{Proof.}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} (Brownian Bridge Process). Suppose that $X_t$ follows the so-called Brownian Bridge process with SDE
$$dX_t = \frac{y-X_t}{1-t}dt + dW_t$$
With $X_1=y$ and the difussion is conditioned at $y$ at time $t=1$.
By applying Itô's formula to $Y_t = (y-X_t)/(1-t)$ and taking integrals show that for $t<T$,
$$X_t = yt + (1-t)\left(x+\int_0^t\frac{1}{1-s}dW_s\right)$$
Find the mean an variance of $X_t| X_0=x$, and show that $X_t$ follows a Normal distribution.
\end{tcolorbox}
\textbf{Proof.}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $X_t$ satisfy the SDE 
$$dX_t = \sqrt{X_t+1}dB_t \quad\quad X_0=0$$
Assuming that the Itô integrals are martingales, find $\mathbb{E}X_t$ and $\mathbb{E}X_t^2$. Let $m(u,t)=\mathbb{E}e^{uX_t}$ be the moment generating function of $X_t$. Show that $m(u,t)$ satisfies
$$\frac{\partial m}{\partial t} = \frac{u^2}{2}\frac{\partial m}{\partial u} + \frac{u^2}{2}m$$
\end{tcolorbox}
\textbf{Proof.}



\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} By defining an appropriate process $V_t, t\geq0$ and letting $X_t = U_tV_t$ solve the SDE
$$dX_t = (X_t+t)dt + (3X_t+B_t^2)dB_t, \quad X_0=1$$
Comment on whether or not it is a diffusion type SDE. 
\end{tcolorbox}
\textbf{Solution.}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Find the stochastic logarithm of $B_t^2+1$
\end{tcolorbox}
\textbf{Solution.}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Find the stochastic logarithm of $X_t = \exp(B_t + t)$
\end{tcolorbox}
\textbf{Solution.} The stochastic logarithm is given by 
$$\mathcal{L}(X_t) = \int_0^t \frac{dX_t}{X_t}$$
Through Itô's formula we find $dX_t = X_t(dB_t + 3/2dt)$. Thus 
\begin{align*}
    \mathcal{L}(X_t) = \int_0^tdB_t + \int_0^t\frac{3}{2}dt = B_t + \frac{3}{2}t
\end{align*}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Show that the solution to the stochastic exponential SDE is unique.
\end{tcolorbox}
\textbf{Proof.}

\newpage
\section{Diffusion Processes}

Diffusion processes are the solutions of Stochastic Differential Equations. The main textbook I refer to \cite{Fima} dedicates an entire chapter to results on Diffusion Processes; though many of the exercises are really just applications of Itô's lemma. For consistency, I list some of the key exercises here so I know where to refer to them.

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Show that for any $u$, $f(x,t) = \exp(ux-u^2t/2)$ solves the backward equation for Brownian Motion. Show that 
$$f_k(x,t) := \frac{d^k}{du^k}\exp(ux-u^2t/2)$$
also solves the backward equation for Brownian Motion.
\end{tcolorbox}
\textbf{Proof.} We consider the backward equation for Brownian Motion. That is, the process $X_t = B_t\implies dX_t=dB_t\implies\mu(X_t,t)=0,\sigma(X_t,t)=1$. The backwards equation is therefore
$$\frac{\partial f}{\partial s}(x,s) + \frac{1}{2}\frac{\partial^2 f}{\partial x^2}(x,s) \quad\quad (1)$$
Taking the partial derivatives of $f(x,s) = \exp(xu - u^2t/2)$ we see that the PDE in $(1)$ is satisfied. $\square$

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $\{X_t\}$ be a diffusion with $\mu(x) = 2x$ and $\sigma^2(x) = 4x$. Write the generator, solve $Lf=0$ and find a martingale.  
\end{tcolorbox}
\textbf{Solution.} The generator is given by
$$Lf(x) = 2x f^\prime(x) + 2xf^{\prime\prime}(x)$$
Setting $Lf=0$ and rearranging we find $f^{\prime \prime}(x) = -f^\prime(x)$. One can do some \textit{correct} things by applying some ODE theorems, but this isn't a class in Differential Calculus so I prefer to just notice that it has to be of the form $f(x) = e^{-x}$. One can easily verify this. Once accounting for possible constants we are left with 
$$f(x) = C_0 + C_1e^{-x}$$
as the solution. To find a martingale, we know that if $\frac{\partial f}{\partial t} + L_f=0$ then the process is a martingale. From here, we deduce that $M_t := e^{-X_t}$ is a martingale. $\square$


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Find an $f$ such that $f(B_t + t)$ is a martingale
\end{tcolorbox}
\textbf{Solution.} Let $X_t = B_t +t\implies dX_t=dB_t+dt$. For $f(X_t)$ to be a martingale we require $df(X_t)$ to have zero drift. By Itô's lemma we must have
$$df(X_t) = \left(f^\prime(X_t) +\frac{1}{2}f^{\prime\prime}(X_t)\right)dt + f^\prime(X_t)dB_t$$
So in order for $0$ drift we must have 
$$f^\prime(X_t) = -\frac{1}{2}f^{\prime\prime}(X_t)$$
From here, we see that $f(x) = C_0e^{-2x} + C_1$ satisfies the above differential equation. $\square$

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $X_t$ be a diffusion with coefficients $\mu(x,t)$ and $\sigma(x,t)$. Find a differential equation for $f(x,t)$ such that $Y_t = f(X_t,t)$ has infintesimal diffusion coefficient equal to 1. 
\end{tcolorbox}
\textbf{Solution.} $dX_t = \mu(X_t,t)dt + \sigma(X_t,t)dB_t$. By Itô's lemma
\begin{align*}
    df(X_t,t) = \frac{\partial}{\partial x} f(X_t,t)\sigma(X_t,t)dB_t + \text{$dt$ terms}
\end{align*}
Since we want the infintesimal coefficient to be equal to one we set 
$$\frac{\partial}{\partial x} f(X_t,t)\sigma(X_t,t) = 1$$
which gives the PDE
$$\frac{\partial}{\partial x} f(X_t,t)= \frac{1}{\sigma(X_t,t)}$$
$\square$

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Suppose that $X_t$ is a diffusion with coefficients $\mu(x) = cx$ and $\sigma(x)=1$. Give its generator and show that $X_t^2-2c\int_0^tX_s^2ds-t$ for $t\geq0$ is a Martingale.
\end{tcolorbox}
\textbf{Solution.}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Suppose $X_t$ is a diffusion process. Find the SDE for the process $Y_t = \sqrt{X_t}$ and give the generator for $Y_t$.
\end{tcolorbox}
\textbf{Solution.}

\todo{Check this in consult. See P-Set 8}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} The following is a multipart problem. A diffusion process is given by the following SDE 
$$dX_t = \mu(X_t,t)dt + \sigma(X_t,t)dB_t$$
\begin{enumerate}[label=(\alph*)]
    \item Find the generator $L_s$ of the process $X_t$
    \item Compute the stochastic differential $df(X_t,t)$ where $f\in C^{2,1}(\mathbb{R}\times[0,\infty))$. 
    \item Under suitable assumptions of smoothness use Itô's formula to show that the following process defines a Martingale:
    $$M_f(t):=f(X_t,t)-\int_0^t(L_sf+\frac{\partial f}{\partial s}(X_s,s)ds.$$
    \item Assume that $f$ satisfies the Backward Kolmogorov Equation. Using part (c), show that $f(X_t,t)$ is a Martingale. State Dynkin's formula for $\mathbb{E}f(X_t,t)$
    \item for $0\leq t\leq T$. Suppose that $f$ solves the Backward Kolmogorov Equation and satisfies $f(x,T)=g(x)$. Show that $f(x,t) = \mathbb{E}[g(X_T)|X_t=x]$
\end{enumerate}
\end{tcolorbox}
\textbf{Solution.} \\
(a) The generator is defined as
$$L_s = \frac{\partial}{\partial x}f(x,s)\mu(x,s) + \frac{1}{2}\frac{\partial^2}{\partial x^2}f(x,s)\sigma(x,s).$$
(b) By Itô's formula, this can be written compactly as 
$$df(X_t,t) = \left(\frac{\partial f}{\partial t}(X_t,t) + L_t(X_t,t)\right)dt + \frac{\sigma(X_t,t)^2}{2}\frac{\partial^2 f}{\partial x^2}(X_t,t)dB_t$$
(c) The intuition is that $$dM_f(t) = df(X_t,t) - (L_tf+\frac{\partial f}{\partial t})(X_t,t)dt$$ 
should not contain any drift, and therefore should have no $dt$ term. From part (b) we see that for a function $f\in C^{2,1}$ the drift term in $df(X_t,t)$ will be precisely $(L_tf+\frac{\partial f}{\partial t})(X_t,t)dt$. Thus $M_f(t)$ effectively \textit{cancels} this drift, resulting in a Martingale. \\
(d)


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} The following is a multipart problem. A time-homogenous diffusion process is given by the following SDE 
$$dX_t = \mu(X_t)dt + \sigma(X_t)dB_t$$
set $P(y,t,x) := \mathbb{P}(X_t\leq y|X_0=x)$ and assume that $p(y,t,x)=\frac{\partial}{\partial y}P(y,t,x)$ exists.
\begin{enumerate}[label=(\alph*)]
    \item Under suitable conditions, write down the backward and forward Kolmogorov equations for the density $p$
\end{enumerate}
\end{tcolorbox}
\textbf{Solution.} \\
(a) 

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Give the generator for the Ornstein-Uhlenbeck process. Write the backward equation and give its fundamental solution.
\end{tcolorbox}
\textbf{Solution.} For this exercise we assume that $dX_t = -\alpha X_tdt + \sigma dB_t$. By applying Ito's lemma to $f(X_t,t)= e^{\alpha t}X_t$ we find the solution to the SDE is 
$$X_t = e^{-\alpha t}\left(X_0 + \int_0^te^{-\alpha s}dB_s\right)\quad\quad(1)$$
From the definition of $dX_t$ we see that the generator for the process is
$$\frac{\partial f}{\partial x^2}\frac{\sigma^2}{2} - \alpha x\frac{\partial f}{\partial x}$$
The fundamental solution is given by 
$$\frac{\partial}{\partial y}\mathbb{P}(X_t\leq y | X_0=x)$$
Given the form of $X_t$ in $(1)$ it is straightforward to calculate this. Note that $X_t$ is an Itô Integral, so $X_t | X_0=x\sim N(e^{-\alpha t}x, \frac{\sigma^2}{2\alpha}(1-e^{-2\alpha t}))$. Thus, 
\begin{align*}
    \frac{\partial}{\partial y}\mathbb{P}(X_t\leq y | X_0=x) &= \frac{\partial}{\partial y}\Phi\left(\frac{y-e^{-\alpha t}x}{\sqrt{\frac{\sigma^2}{2\alpha}(1-e^{-2\alpha t})}}\right) \\
    &= \frac{1}{\sqrt{\frac{\sigma^2}{2\alpha}(1-e^{-2\alpha t})}}\phi\left(\frac{y-e^{-\alpha t}x}{\sqrt{\frac{\sigma^2}{2\alpha}(1-e^{-2\alpha t})}}\right)
\end{align*}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $X_t$ be a diffusion process with $\sigma(X_t) = 1$ and 
\[\mu(X_t) = \begin{cases}
    -1 & x>0 \\
    +1 & x<0 \\
    0 & \text{otherwise.}
\end{cases}\]
Show that $\pi(x) = e^{-2 |x|}$ is a stationary distribution of $X_t$.
\end{tcolorbox}
\textbf{Proof.}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Give a probabilistic representation of the soltuion $f(x,t)$ to the PDE
$$\frac{1}{2}\frac{\partial^2 f}{\partial x^2} + \frac{\partial f}{\partial t} =0 \quad 0\leq t\leq T\quad f(x,T)=x^2$$
Then solve the PDE using the corresponding solution to the SDE.
\end{tcolorbox}
\textbf{Solution.} This is an application of Feynman-Kac. The PDE is of the form 
$$\frac{\partial f }{\partial t} + L_f=0$$
where $L_f = \frac{1}{2}\frac{\partial^2 f}{\partial x^2}$. Such a generator corresponds to the SDE $dX_t = dB_t$. The Faynmin-Kac formula tells us that the solution to such an SDE is 
$$f(x,t) = \mathbb{E}[g(X_T) | X_t=x] \quad\quad g(x)=x^2\quad\quad(1)$$
So the full, corresponding SDE is $dX_t = dB_t$ with $X_t=x$ on $t\leq T$. Solving this SDE we know that $X_T = x + B_T-B_t\sim N(x,T-t)$ so the solution to $(1)$ is:
$$f(x,t) = \mathbb{E}[X_T^2 | X_t=x] = x^2 + (T-t)$$
$\square$

\newpage
\section{Change of Probability Measure}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Show that if $Q$ is equivalent to $P$ and $X\geq0$ then $E_PX > 0 $ implies $E_QX>0$ and vice-versa. 
\end{tcolorbox}
\textbf{Proof.} Since $Q\sim P$ then there exists a Radon-Nikodym derivative $\Lambda>0$. Recall that 
$$\mathbb{E}_QX = \mathbb{E}_P\Lambda X$$
Suppose $\mathbb{E}_PX >0$ then there exists a set $A$ such that $X(\omega)>0$ on $A$ and $P(A)>0$. We then have
$$\mathbb{E}_QX = \mathbb{E}_P\Lambda X \geq \mathbb{E}_P\Lambda X\cdot1_A>0$$
On the converse, suppose $\mathbb{E}_Q X> 0$ Then $\mathbb{E}_P\Lambda X>0$ and since $\Lambda > 0$ and $X\geq 0$ we must have $X>0$ ona a set of positive $P$-measure and we conclude $\mathbb{E}_PX>0$ as well. $\square$



\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Define $W_t = \mu t + B_t$ for a $\mathbb{P}$-Brownian Motion $B_t$. Define $W_t^*:=\max_{s\leq t} W_s$. Find the distribution of $W_t^*$
\end{tcolorbox}
\textbf{Solution.} The intuition is that we can't follow the same argument for finding the maximum of Brownian Motion, as $W_t$ is no longer a martingale. Instead, we'll apply a change of Measure to swap to an equivalent probability measure where $\mathbb{Q}$ it is a regular Brownian Motion, and then change back to $\mathbb{P}$-measure. \\
\\
The first step is to apply Girsanov's theorem to identify that 
$$\Lambda = \frac{d\mathbb{Q}}{d\mathbb{P}} = \exp\left(-\mu B_t -\frac{\mu^2 t}{2}\right)$$
defines the appropriate Radon-Nikodym derivative to swap to an equivalent measure $\mathbb{Q}$ that removes the drift from $W_t$. Since $W_t$ is a $\mathbb{Q}$-Brownian Motion, we can apply our earlier results to find\footnote{See joint distribution of Brownian Motion and its maximum in Chapter 3.}
\begin{align*}
    \mathbb{Q}(\max_{s\leq t} W_t\geq y, W_t\leq x) &= 1-\Phi\left(\frac{2y-x}{\sqrt{t}}\right)
\end{align*}
We can obtain the $\mathbb{Q}$-density by differentiating
\begin{align*}
    q_{\max W_t, W_t}(y,x) &= \frac{\partial}{\partial y\partial x} \left[1-\Phi\left(\frac{2y-x}{\sqrt{t}}\right)\right] \\
    &= \sqrt{\frac{2}{\pi}}\frac{2y-x}{t^{3/2}}\exp\left(-\frac{1}{t}(2y-x)^2\right)
\end{align*}
Which is of course defined for $y\geq 0, x\leq y$. We now need to swap back to $\mathbb{P}$-measure and integrate out $W_t$ to find a density for $\max_{s\leq t} W_s$. Define $A = \{x\leq y: M_t{\leq y, B_{t}\leq x}\}$ then 
\begin{align*}
    p_{\max_{s\leq t}W_s}(y) = \int_Ad\mathbb{P} &= \int_A\Lambda^{-1}d\mathbb{Q} \\
    &= \int_{-\infty}^{x}\Lambda^{-1}q_{\max W_t, W_t}(x,y)dx.
\end{align*}
A subtle, but important point is that $\Lambda^{-1} = \frac{d\mathbb{P}}{d\mathbb{Q}}$, is for swapping from $\mathbb{Q}$ to $\mathbb{P}$, so it must be expressed in terms of the $\mathbb{Q}$-Brownian Motion
\begin{align*}
    \Lambda^{-1} = \exp\left(\mu B_t^\mathbb{P} - \frac{\mu^2 t}{2}\right) &= \exp\left(\mu (W_t-\mu t) + \frac{\mu^2 t}{2}\right) \\
    &=\exp\left(\mu W_t - \frac{\mu^2 t}{2}\right).
\end{align*}
Thus the density can be written as 
$$p_{\max_{s\leq t}W_s}(y) = \int_{-\infty}^x \exp\left(\mu x - \frac{\mu^2 t}{2}\right)\sqrt{\frac{2}{\pi}}\frac{2y-x}{t^{3/2}}\exp\left(-\frac{1}{t}(2y-x)^2\right)dx$$
By setting $z:= 2y-x \implies dx=-dz$ one can massage the above into a nicer form. $\square$


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Give the Radon-Nikodym Derivative in the change to the EMM $\mathbb{Q}$ in the Black-Scholes model. 
\end{tcolorbox}
\textbf{Solution.}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $\mathbb{P}$ and $\mathbb{Q}$ be probability measures induced by $N(\mu_1,1)$ and $N(\mu_2,1)$ respectively. Show that $\mathbb{P}\sim\mathbb{Q}$. Find $\frac{d\mathbb{Q}}{d\mathbb{P}}$ and $\frac{d\mathbb{P}}{d\mathbb{Q}}$.
\end{tcolorbox}
\textbf{Solution.}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Show that if a random variable $X$ on $(\Omega, \mathcal{F}, \mathbb{P})$ follows a $N(\mu,1)$ distribution, then show there is an equivalent measure $\mathbb{Q}$ such that $X$ follows a $N(0,1)$ distribution on $(\Omega, \mathcal{F},\mathbb{Q}).$ State the Radon-Nikodym derivatives $\frac{d\mathbb{Q}}{d\mathbb{P}}$ and $\frac{d\mathbb{P}}{d\mathbb{Q}}$. What distribution does $Y:=X-\mu$ follow under $\mathbb{Q}$?
\end{tcolorbox}
\textbf{Solution.}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} A random variable $Y$ has lognormal LN$(\mu,\sigma^2)$
distribution under probability measure $P$. Let $\Lambda = Y/E_P[Y]$ so that $dQ(\omega) = \Lambda(Y(\omega))dP(\omega)$. 
\begin{enumerate}[label=(\alph*)]
    \item Write $E_P[Y\cdot 1_{Y>K}]$ in terms of $Q(Y>K)$
    \item Let $X := \ln Y$ find the distribution of $X$ under $Q$. 
    \item Now find $E_P[Y\cdot 1_{Y>K}]$ in terms of the CDF of $X$ from part (b)
\end{enumerate}
\end{tcolorbox}
\textbf{Solution.} \\
(a) We have 
\begin{align*}
    E_P[Y \cdot 1_{Y>K}] = E_Q[\Lambda^{-1} Y \cdot1_{Y>K}] &= E_P[E_P[Y]\cdot 1_{Y>K}] \\
    &=E_P[Y]Q(Y>K)
\end{align*}
since $Y\sim\text{LN}(\mu,\sigma^2)$ one can write $E_P[Y\cdot1_{Y>K}] = e^{\mu+\frac{\sigma^2}{2}}Q(Y>K)$. $\square$ \\
\\
(b) Consider the moment generating function $Ee^{uX}$ to identify the distribution of $X$. Under $Q$
\begin{align*}
    E_Qe^{uX} = E_P\Lambda e^{uX} = e^{-\mu-\frac{\sigma^2}{2}} E_P Y e^{uX}
\end{align*} 
Since $Y = e^X$ in $P$ we write 
\begin{align*}
    e^{-\mu-\frac{\sigma^2}{2}} E_P Y e^{uX} &= e^{-\mu-\frac{\sigma^2}{2}} E_P e^{(u+1)X} \\
    &= e^{-\mu-\frac{\sigma^2}{2}} e^{\mu(u+1) + \frac{\sigma^2}{2}(u+1)^2} \\
    &= e^{(\mu+\sigma^2)u + \frac{\sigma^2}{2}u^2}
\end{align*} 
Which matches the moment generating function of a $N(\mu+\sigma^2, \sigma^2)$ distribution. $\square$ \\
\\
(c) Since $E_P[Y\cdot1_{Y>K}] = e^{\mu+\frac{\sigma^2}{2}}Q(Y>K)$ and $Y\sim \text{LN}(\mu+\sigma^2,\sigma^2)$ in $Q$ we can write 
\begin{align*}
    Q(Y>K) = Q(X>\ln K)  &= Q\left(Z>\frac{\ln K-\mu-\sigma^2}{\sigma}\right)
\end{align*}
Using $1-\Phi(x) = \Phi(-x)$ where $\Phi$ denotes the normal CDF, we can write 
$$E_P[Y\cdot1_{Y>K}] = e^{\mu+\sigma^2/2}\Phi\left(\frac{\mu+\sigma^2-\ln K}{\sigma}\right)$$
 
\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $X_t = B_t + \sin(t)$, for a $P$-Brownian Motion. Let $Q\sim P$ be another probability measure such that $X_t$ is a Q-Brownian Motion. Find $\Lambda = \frac{dQ}{dP}$.
\end{tcolorbox}
\textbf{Solution.} we have $dX_t = dB_t + \cos(t)dt$. We require $dX_t$ be a martingale under $Q$. Girsanov Theorem gives us an explicit form for the Radon-Nikodym derivative to remove the drift of $dX_t$ under $Q$. Let $\theta _t := \cos(t)$ then 
\begin{align*}
    \Lambda &= \exp\left(-\int_0^t\theta_sdB_s - \frac{1}{2}\int_0^t\theta_s^2ds\right) \\
    &= \exp\left(-\int_0^t \cos sdB_s - \frac{1}{2}\int_0^t\cos^2s ds\right)
\end{align*}
Defines the Radon-Nikdoym derivative that transforms $X_t$ to a $Q$-Brownian Motion. $\square$

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $X_t = B_t + t^2$, for a $P$-Brownian Motion. Let $Q\sim P$ be another probability measure such that $X_t$ is a Q-Brownian Motion. Find $\Lambda = \frac{dQ}{dP}$.
\end{tcolorbox}
\textbf{Solution.}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $X_t = B_t + \int_0^t B_sds$, for a $P$-Brownian Motion. Let $Q\sim P$ be another probability measure such that $X_t$ is a Q-Brownian Motion. Find $\Lambda = \frac{dQ}{dP}$.
\end{tcolorbox}
\textbf{Solution.}




\newpage
\section{Applications to Finance}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} In the Binomial Pricing Model, show that the stock price can be expressed $S_t = \xi_tS_{t-1}$ where the $\xi_i$ are independent and identically distributed.   \cite{Fima}
\end{tcolorbox}
\textbf{Solution.}
Recall that in the Binomial Pricing Model, the stock can transition either $S_{t} \to uS_t$ or $S_t \to dS_t$. Define the random variable $\xi_i$ according to 
\[ \xi_i = \begin{cases} 
      u & \frac{u-r}{u-d} \\
      d & \frac{r-d}{u-d}
   \end{cases}
\]
Then it is clear that $S_{t+1} = S_0\prod_{i=1}^{t}\xi_i$. $\square$

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Show that the Binomial Model does not admit arbitrage $\iff$ $d<r<u.$
\end{tcolorbox}
\textbf{Proof.}
$(\Rightarrow)$. Suppose the model does not admit arbitrage. Then by the first fundamental theorem of asset pricing, there exists a probability measure $\mathbb{Q}$ such that the process $S_t / r^t$ is a martingale. Define $p:= \frac{u-r}{u-d}$ then we have 
\begin{align*}
    E_\mathbb{Q}\left[\frac{S_t}{r^t} | \mathcal{F}_{t-1}\right]
\end{align*}
 
\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Define what it means for a Market-Model to be \textit{Complete}. Show that the Binomial Model is complete. 
\end{tcolorbox}
\textbf{Solution.} A model is \textit{complete} if any claim can be replicated. That is, there exists a replicating portfolio for every possible payoff. In the Binomial model, the only payoffs are $C_u, C_d$ which are the result of the stock price moving to $uS$ and $dS$ respectively. We set the simultaneous equations 
\begin{align*}
    & auS + br = C_u \\
    & adS + br = C_d
\end{align*}
equal to each other.


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Give an example of a model which is not complete. 
\end{tcolorbox}
\textbf{Solution.}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Define the discounted price process. Show that in the Black-Scholes model, the discounted price-process is a martingale if and only if $\mu = r$.
\end{tcolorbox}
\textbf{Solution.} The discounted stock price process is given by $Z_t := e^{-rt}S_t$. By Applying Ito's formula to $f(x,t) = e^{-rt}x$ we see that 
$$dZ_t = e^{-rt}S_t(\sigma dB_t + (\mu-r)dt)$$
In order for $Z_t$ to be a martingale, it must be driftless which we see is only possible if and only if $\mu = r$ vanishing the $dt$ term.


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Let $S_1$ and $S_2$ be two non-dividend paying stocks that evolving according to geometric Brownian motion. Find a stochastic differential for $S_1 + S_2$ when $S_1$ and $S_2$ are independent, and when they share correlation equivalent $\rho$.  \cite{Hull}
\end{tcolorbox}
\textbf{Solution.}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} (Black-Scholes Via the EMM). Using an equivalent martingale measure and assumptions of the Black-Scholes model, show that the price of a Call option struck at $K$ satisfies
$$C_t = S_t\Phi(h_t) - Ke^{-r(T-t)}\Phi(h_t-\sigma\sqrt{T-t})$$
where 
$$h_t = \frac{\ln\frac{S_t}{K} + (r+\frac{1}{2}\sigma^2)(T-t)}{\sigma\sqrt{T-t}}$$
\end{tcolorbox}
\textbf{Proof.}

\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} (Black-Scholes Via the PDE). 
\end{tcolorbox}
\textbf{Proof.}


\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Suppose $S_t$ follows geometric brownian motion and let $G$ be a function of $S_t$ and $t$. Show that when the expected return of $S_t$ increases by $\lambda \sigma _S$ the expected return of $G$ grows by $\lambda \sigma _G$. \cite{Hull}
\end{tcolorbox}
\textbf{Proof.} To begin, note that $dS_t = \mu S_t dt + \sigma_S S_tdB_t$. We now apply Itô's lemma so that 
\begin{align*}
   dG &= \frac{\partial G}{\partial S_t}dS_t + \frac{\partial G}{\partial t}dt + \frac{\partial^2 G}{\partial S_t^2}\sigma^2_s S_t^2dt  \\
   &= \frac{\partial G}{\partial S_t}(\mu S_t dt + \sigma_sS_tdB_t) + \frac{\partial G}{\partial t}dt + \frac{\partial^2 G}{\partial S_t^2}\sigma^2_s S_t^2dt \\
   &= \left(\frac{\partial G}{\partial S_t}\mu S_t + \frac{\partial G}{\partial t} + \frac{\partial^2 G}{\partial S_t^2}\sigma_S^2S_t^2\right)dt + \frac{\partial G}{\partial S_t}\sigma_S S_t dB_t
\end{align*}
From here one sees that 
\begin{align*}
     & \sigma_G = \frac{\partial G}{\partial S_t}\sigma_S S_t \\
    & \mu_G  = \frac{\partial G}{\partial S_t}\mu S_t + \frac{\partial G}{\partial t} + \frac{\partial^2 G}{\partial S_t^2}\sigma_S^2S_t^2
\end{align*}
The \textit{expected return of $S_t$} is controlled entirely through $\mu$. So to increase the expected rate of return by $\lambda\sigma_S$ we update the mean to $\mu + \lambda\sigma_S$. Thus, $\mu_G$ is increased by
$$\frac{\partial G}{\partial S_t}\lambda\sigma_S S_t = \lambda\sigma_G$$
as claimed. $\square$
 
\begin{tcolorbox}[colframe=black,colback=gray!5,boxrule=0.5pt]
\textbf{Question:} Derive the price evolution of a Zero Coupon Bond paying \$1 at time $T$, given the yield $X_t$ on the bond evolves according to $dX_t = \alpha(X_t-X_0)dX_t + \sigma X_tdB_t$. \cite{Hull}
\end{tcolorbox}
\textbf{Proof}. The price of the bond at time $t\leq T$ is $P_t = e^{-X_t(T-t)}$. The trick is to now apply Itô's lemma to $f(s) = e^{-s(T-t)}$.
\newpage
\bibliographystyle{plain} 
\bibliography{references}  % Assumes a references.bib file

\end{document}
